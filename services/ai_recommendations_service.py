"""
Spotify-Inspired AI Recommendations Service
Advanced personalized paper discovery system

This service provides Spotify-like AI-enhanced features including:
1. "Papers for You" - Personalized daily feed based on user behavior
2. "Trending in Your Field" - Hot topics and emerging research
3. "Cross-pollination" - Interdisciplinary discovery opportunities
4. "Citation Opportunities" - Papers that could cite your work
5. Weekly recommendation mixes generated every Monday
6. User-specific learning and preference modeling
7. Collection-specific recommendations
"""

import asyncio
import logging
import hashlib
import random
import os
from typing import List, Dict, Any, Optional, Tuple, Set
from datetime import datetime, timedelta, timezone
from collections import defaultdict, Counter
import json
import re

from database import get_db, Article, Collection, Project, User, ArticleCollection
from sqlalchemy.orm import Session
from sqlalchemy import func, desc, and_, or_, text

# Import AI agents for enhanced recommendations
try:
    from recommendation_agents import RecommendationOrchestrator
    from langchain_openai import ChatOpenAI
    AI_AGENTS_AVAILABLE = True
except ImportError as e:
    # Logger not yet defined, use print for now
    print(f"AI agents not available: {e}")
    AI_AGENTS_AVAILABLE = False

# Import semantic analysis service
try:
    from services.semantic_analysis_service import SemanticAnalysisService
    SEMANTIC_ANALYSIS_AVAILABLE = True
except ImportError as e:
    print(f"Semantic analysis service not available: {e}")
    SEMANTIC_ANALYSIS_AVAILABLE = False

logger = logging.getLogger(__name__)

class SpotifyInspiredRecommendationsService:
    """Spotify-inspired AI-powered paper recommendations and discovery service"""

    def __init__(self):
        self.recommendation_cache = {}
        self.semantic_cache = {}  # Cache for semantic analysis results

        # Initialize semantic analysis service
        self.semantic_service = None
        if SEMANTIC_ANALYSIS_AVAILABLE:
            try:
                self.semantic_service = SemanticAnalysisService()
                logger.info("‚úÖ Semantic analysis service initialized")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize semantic analysis service: {e}")
                self.semantic_service = None

        # Initialize AI agents if available
        self.ai_orchestrator = None
        if AI_AGENTS_AVAILABLE:
            try:
                # Initialize LLM for AI agents
                api_key = os.getenv("OPENAI_API_KEY")
                if api_key:
                    llm = ChatOpenAI(
                        model="gpt-4o-mini",
                        openai_api_key=api_key,
                        temperature=0.3
                    )
                    self.ai_orchestrator = RecommendationOrchestrator(llm)
                    logger.info("‚úÖ AI recommendation agents initialized successfully")
                else:
                    logger.warning("‚ö†Ô∏è OPENAI_API_KEY not found, AI agents disabled")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize AI agents: {e}")
                self.ai_orchestrator = None
        self.user_behavior_cache = {}
        self.cache_ttl = timedelta(hours=6)  # Cache recommendations for 6 hours
        self.behavior_cache_ttl = timedelta(hours=1)  # Cache user behavior for 1 hour

        # Spotify-like recommendation categories
        self.recommendation_categories = {
            "papers_for_you": "Personalized daily feed",
            "trending_in_field": "Hot topics in your research area",
            "cross_pollination": "Interdisciplinary discoveries",
            "citation_opportunities": "Papers that could cite your work"
        }

        # User interaction weights for learning
        self.interaction_weights = {
            "saved_to_collection": 1.0,
            "network_exploration": 0.7,
            "time_spent_reading": 0.5,
            "citation_added": 0.9,
            "shared_paper": 0.8,
            "deep_dive_analysis": 1.2
        }

    async def _get_direct_user_recommendations(self, user_id: str, project_id: Optional[str], db: Session) -> Optional[Dict[str, Any]]:
        """Direct approach to get recommendations from user's collection articles"""
        try:
            logger.info(f"üéØ Direct approach: Getting recommendations for user: {user_id}")

            # Get user's collections directly
            user_collections = db.query(Collection).filter(
                Collection.created_by == user_id
            ).all()

            logger.info(f"üìö Found {len(user_collections)} collections for user")

            # Get all articles from user's collections
            user_articles = []
            for collection in user_collections:
                articles_in_collection = db.query(ArticleCollection).filter(
                    ArticleCollection.collection_id == collection.collection_id
                ).all()
                user_articles.extend(articles_in_collection)
                logger.info(f"üìÑ Collection '{collection.collection_name}' has {len(articles_in_collection)} articles")

            logger.info(f"üìä Total user articles found: {len(user_articles)}")

            # If no articles found, try user's subject area or generate intelligent recommendations
            if not user_articles:
                logger.info("üìä No user articles found, trying user subject area")
                from database import User
                user_record = db.query(User).filter(
                    or_(User.email == user_id, User.user_id == user_id)
                ).first()

                if user_record and user_record.subject_area:
                    logger.info(f"üéì Found user's subject area: {user_record.subject_area}, will use in profile building")
                    # Don't return recommendations here, let the normal flow handle it with subject area info
                    return None  # This will trigger the normal profile building with subject area
                else:
                    logger.info("üìä No subject area found, generating intelligent general recommendations")
                    return None  # This will trigger the normal fallback flow

            # Extract research domains from user's articles
            research_domains = set()
            for article in user_articles:
                title_lower = (article.article_title or "").lower()

                # Detect domains from article titles
                if any(term in title_lower for term in ['kidney', 'renal', 'nephrology']):
                    research_domains.add('nephrology')
                if any(term in title_lower for term in ['diabetes', 'diabetic', 'glucose']):
                    research_domains.add('diabetes')
                if any(term in title_lower for term in ['cardiovascular', 'heart', 'cardiac']):
                    research_domains.add('cardiovascular')
                if any(term in title_lower for term in ['finerenone', 'mineralocorticoid', 'pharmacology']):
                    research_domains.add('pharmacology')
                if any(term in title_lower for term in ['cancer', 'tumor', 'oncology']):
                    research_domains.add('oncology')
                if any(term in title_lower for term in ['treatment', 'therapy', 'clinical']):
                    research_domains.add('clinical_medicine')

            research_domains = list(research_domains)
            logger.info(f"üî¨ Detected research domains: {research_domains}")

            # Generate recommendations based on detected domains
            raw_recommendations = await self._generate_domain_based_recommendations(research_domains, db)

            # üö® CRITICAL: Convert structure for global deduplication compatibility
            # Direct approach returns {section: [papers]} but deduplication expects {section: {papers: [papers]}}
            structured_recommendations = {}
            for section, papers in raw_recommendations.items():
                structured_recommendations[section] = {"papers": papers}

            # üö® CRITICAL: Apply global deduplication to direct recommendations
            logger.info("üîÑ Applying global deduplication to direct user recommendations...")
            deduplicated_structured = self._apply_global_deduplication(structured_recommendations)
            logger.info(f"‚úÖ Global deduplication applied to direct recommendations")

            # Convert back to expected format for direct approach
            recommendations = {}
            for section, data in deduplicated_structured.items():
                recommendations[section] = data.get("papers", [])

            return {
                "recommendations": recommendations,
                "user_insights": {
                    "research_domains": research_domains,
                    "activity_level": "active" if len(user_articles) > 5 else "moderate",
                    "discovery_preference": "balanced",
                    "collaboration_tendency": 0.7
                }
            }

        except Exception as e:
            logger.error(f"‚ùå Error in direct user recommendations: {e}")
            return None

    def _get_week_start(self) -> datetime:
        """Get the start of the current week (Monday)"""
        today = datetime.now(timezone.utc)
        days_since_monday = today.weekday()
        week_start = today - timedelta(days=days_since_monday)
        return week_start.replace(hour=0, minute=0, second=0, microsecond=0)

    async def _generate_domain_based_recommendations(self, research_domains: List[str], db: Session) -> Dict[str, Any]:
        """Generate recommendations based on detected research domains"""
        try:
            recommendations = {
                "papers_for_you": [],
                "trending_in_field": [],
                "cross_pollination": [],
                "citation_opportunities": []
            }

            # Get all available articles from the database
            all_articles = db.query(Article).limit(100).all()  # Get a reasonable sample
            logger.info(f"üìä Found {len(all_articles)} articles in database for recommendations")

            if not all_articles:
                # If no articles in Article table, generate fallback
                return {
                    "papers_for_you": await self._generate_fallback_recommendations(db, "papers_for_you"),
                    "trending_in_field": await self._generate_fallback_recommendations(db, "trending_in_field"),
                    "cross_pollination": await self._generate_fallback_recommendations(db, "cross_pollination"),
                    "citation_opportunities": await self._generate_fallback_recommendations(db, "citation_opportunities")
                }

            # Generate Papers for You based on user's domains (filter out invalid metadata)
            papers_for_you = []
            for domain in research_domains[:3]:  # Top 3 domains
                domain_papers = [
                    article for article in all_articles
                    if (article.title and
                        article.title.strip() and
                        not article.title.startswith("Citation Article") and
                        not article.title.startswith("Reference Article") and
                        (domain.lower() in article.title.lower() or
                         domain.lower() in (article.abstract or "").lower()))
                ][:8]  # 8 papers per domain

                for paper in domain_papers:
                    papers_for_you.append({
                        "pmid": paper.pmid,
                        "title": paper.title,
                        "authors": paper.authors[:3] if paper.authors else [],
                        "journal": paper.journal,
                        "year": paper.publication_year,
                        "citation_count": paper.citation_count or 0,
                        "relevance_score": 0.9,
                        "reason": f"Matches your research in {domain}",
                        "category": "papers_for_you"
                    })

            # Generate Trending in Field (filter out invalid metadata)
            valid_articles = [
                article for article in all_articles
                if (article.title and
                    article.title.strip() and
                    not article.title.startswith("Citation Article") and
                    not article.title.startswith("Reference Article"))
            ]
            trending_papers = sorted(valid_articles, key=lambda x: x.citation_count or 0, reverse=True)[:15]
            trending_in_field = []
            for paper in trending_papers:
                trending_in_field.append({
                    "pmid": paper.pmid,
                    "title": paper.title,
                    "authors": paper.authors[:3] if paper.authors else [],
                    "journal": paper.journal,
                    "year": paper.publication_year,
                    "citation_count": paper.citation_count or 0,
                    "relevance_score": 0.8,
                    "reason": "Highly cited in your field",
                    "category": "trending_in_field"
                })

            # Generate Cross-pollination (papers from different domains, filter out invalid metadata)
            cross_pollination = []
            other_domains = ['machine learning', 'artificial intelligence', 'bioinformatics', 'genetics']
            for domain in other_domains:
                domain_papers = [
                    article for article in all_articles
                    if (article.title and
                        article.title.strip() and
                        not article.title.startswith("Citation Article") and
                        not article.title.startswith("Reference Article") and
                        domain.lower() in article.title.lower())
                ][:5]  # 5 papers per cross-domain

                for paper in domain_papers:
                    cross_pollination.append({
                        "pmid": paper.pmid,
                        "title": paper.title,
                        "authors": paper.authors[:3] if paper.authors else [],
                        "journal": paper.journal,
                        "year": paper.publication_year,
                        "citation_count": paper.citation_count or 0,
                        "relevance_score": 0.7,
                        "reason": f"Cross-domain insights from {domain}",
                        "category": "cross_pollination"
                    })

            # Generate Citation Opportunities (recent papers with moderate citations, filter out invalid metadata)
            citation_opportunities = []
            # Try different citation ranges to get more opportunities
            for min_citations, max_citations in [(5, 50), (10, 100), (1, 200), (0, 500)]:
                recent_papers = [
                    article for article in all_articles
                    if ((article.publication_year or 0) >= 2018 and  # Expanded date range
                        min_citations <= (article.citation_count or 0) <= max_citations and
                        article.title and
                        article.title.strip() and
                        not article.title.startswith("Citation Article") and
                        not article.title.startswith("Reference Article"))
                ][:15]

                if len(recent_papers) >= 8:  # If we have enough papers, break
                    break

            for paper in recent_papers:
                citation_opportunities.append({
                    "pmid": paper.pmid,
                    "title": paper.title,
                    "authors": paper.authors[:3] if paper.authors else [],
                    "journal": paper.journal,
                    "year": paper.publication_year,
                    "citation_count": paper.citation_count or 0,
                    "relevance_score": 0.6,
                    "reason": "Good citation opportunity",
                    "category": "citation_opportunities"
                })

            return {
                "papers_for_you": papers_for_you[:20],  # Spotify-style abundance
                "trending_in_field": trending_in_field[:15],
                "cross_pollination": cross_pollination[:15],
                "citation_opportunities": citation_opportunities[:15]
            }

        except Exception as e:
            logger.error(f"‚ùå Error generating domain-based recommendations: {e}")
            # Return fallback recommendations
            return {
                "papers_for_you": await self._generate_fallback_recommendations(db, "papers_for_you"),
                "trending_in_field": await self._generate_fallback_recommendations(db, "trending_in_field"),
                "cross_pollination": await self._generate_fallback_recommendations(db, "cross_pollination"),
                "citation_opportunities": await self._generate_fallback_recommendations(db, "citation_opportunities")
            }

    async def get_weekly_recommendations(self, user_id: str, project_id: Optional[str] = None, force_refresh: bool = False) -> Dict[str, Any]:
        """
        Generate Spotify-style weekly recommendations with 4 main categories

        Args:
            user_id: User identifier
            project_id: Optional project context (if None, generates user-level recommendations)

        Returns:
            Dictionary with weekly recommendations in Spotify format
        """
        try:
            db_gen = get_db()
            db = next(db_gen)

            # Check cache first (unless force refresh is requested)
            cache_key = f"weekly_{user_id}_{project_id or 'global'}"
            if not force_refresh and cache_key in self.recommendation_cache:
                cached_data = self.recommendation_cache[cache_key]
                if datetime.now(timezone.utc) - cached_data["timestamp"] < self.cache_ttl:
                    return cached_data["data"]

            # DIRECT APPROACH: Try to get user articles directly first
            direct_recommendations = await self._get_direct_user_recommendations(user_id, project_id, db)
            if direct_recommendations:
                logger.info(f"‚úÖ Using direct user recommendations approach")
                result = {
                    "status": "success",
                    "week_of": self._get_week_start().isoformat(),
                    "user_id": user_id,
                    "project_id": project_id,
                    "recommendations": direct_recommendations["recommendations"],
                    "user_insights": direct_recommendations["user_insights"],
                    "generated_at": datetime.now(timezone.utc).isoformat(),
                    "next_update": (datetime.now(timezone.utc) + timedelta(days=7)).isoformat()
                }

                # Cache the result
                self.recommendation_cache[cache_key] = {
                    "data": result,
                    "timestamp": datetime.now(timezone.utc)
                }

                return result

            # Get comprehensive user behavior analysis (fallback)
            user_profile = await self._build_user_research_profile(user_id, project_id, db)

            # Try AI-enhanced recommendations first
            if self.ai_orchestrator:
                try:
                    # Get available papers pool for AI agents
                    available_papers = await self._get_available_papers_pool(user_id, project_id, db)

                    # Use AI agents for enhanced recommendations
                    ai_result = await self.ai_orchestrator.generate_all_recommendations(
                        user_profile, available_papers
                    )

                    # Check if AI agents actually returned useful results
                    total_ai_papers = (
                        len(ai_result.get("papers_for_you", [])) +
                        len(ai_result.get("trending", [])) +
                        len(ai_result.get("cross_pollination", [])) +
                        len(ai_result.get("citation_opportunities", []))
                    )

                    if total_ai_papers > 0:
                        # Use AI recommendations if successful
                        raw_recommendations = {
                            "papers_for_you": ai_result.get("papers_for_you", []),
                            "trending_in_field": ai_result.get("trending", []),
                            "cross_pollination": ai_result.get("cross_pollination", []),
                            "citation_opportunities": ai_result.get("citation_opportunities", [])
                        }

                        # Apply global deduplication across all recommendation types
                        recommendations = self._apply_global_deduplication(raw_recommendations)

                        logger.info(f"‚úÖ AI-enhanced recommendations generated for user {user_id} ({total_ai_papers} total papers)")
                    else:
                        logger.warning(f"‚ö†Ô∏è AI agents returned empty results, falling back to traditional methods")
                        raise Exception("AI agents returned no papers, using fallback")

                    # üß† SEMANTIC ANALYSIS ENHANCEMENT FOR AI RECOMMENDATIONS
                    logger.info("üß† Enhancing AI recommendations with semantic analysis...")

                    # Enhance each category with semantic analysis (AI recommendations return lists directly)
                    if recommendations.get("papers_for_you") and isinstance(recommendations["papers_for_you"], list):
                        recommendations["papers_for_you"] = await self._enhance_papers_with_semantic_analysis(
                            recommendations["papers_for_you"]
                        )

                    if recommendations.get("trending_in_field") and isinstance(recommendations["trending_in_field"], list):
                        recommendations["trending_in_field"] = await self._enhance_papers_with_semantic_analysis(
                            recommendations["trending_in_field"]
                        )

                    if recommendations.get("cross_pollination") and isinstance(recommendations["cross_pollination"], list):
                        recommendations["cross_pollination"] = await self._enhance_papers_with_semantic_analysis(
                            recommendations["cross_pollination"]
                        )

                    if recommendations.get("citation_opportunities") and isinstance(recommendations["citation_opportunities"], list):
                        recommendations["citation_opportunities"] = await self._enhance_papers_with_semantic_analysis(
                            recommendations["citation_opportunities"]
                        )

                    logger.info("‚úÖ AI recommendations semantic analysis enhancement completed")

                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è AI agents failed, falling back to traditional method: {e}")
                    # Fallback to traditional methods with proper structure extraction and global deduplication
                    used_pmids = set()
                    logger.info(f"üîÑ AI fallback: Starting global deduplication with empty used_pmids: {len(used_pmids)}")

                    raw_method_results = {}

                    # Generate papers in priority order to ensure diversity
                    raw_method_results["papers_for_you"] = await self._generate_papers_for_you(user_profile, db, used_pmids)
                    if raw_method_results["papers_for_you"].get("papers"):
                        new_pmids = [p.get("pmid") for p in raw_method_results["papers_for_you"]["papers"] if p.get("pmid")]
                        used_pmids.update(new_pmids)
                        logger.info(f"üìÑ AI fallback Papers for You added PMIDs: {new_pmids}, total used: {len(used_pmids)}")

                    raw_method_results["trending_in_field"] = await self._generate_trending_in_field(user_profile, db, used_pmids)
                    if raw_method_results["trending_in_field"].get("papers"):
                        new_pmids = [p.get("pmid") for p in raw_method_results["trending_in_field"]["papers"] if p.get("pmid")]
                        used_pmids.update(new_pmids)
                        logger.info(f"üî• AI fallback Trending added PMIDs: {new_pmids}, total used: {len(used_pmids)}")

                    raw_method_results["cross_pollination"] = await self._generate_cross_pollination(user_profile, db, used_pmids)
                    if raw_method_results["cross_pollination"].get("papers"):
                        new_pmids = [p.get("pmid") for p in raw_method_results["cross_pollination"]["papers"] if p.get("pmid")]
                        used_pmids.update(new_pmids)
                        logger.info(f"üî¨ AI fallback Cross-pollination added PMIDs: {new_pmids}, total used: {len(used_pmids)}")

                    raw_method_results["citation_opportunities"] = await self._generate_citation_opportunities(user_profile, db, used_pmids)
                    if raw_method_results["citation_opportunities"].get("papers"):
                        new_pmids = [p.get("pmid") for p in raw_method_results["citation_opportunities"]["papers"] if p.get("pmid")]
                        used_pmids.update(new_pmids)
                        logger.info(f"üí° AI fallback Citation opportunities added PMIDs: {new_pmids}, total used: {len(used_pmids)}")

                    # Extract papers arrays from method results (methods return {title, papers, updated})
                    raw_recommendations = {}
                    for category, method_result in raw_method_results.items():
                        if isinstance(method_result, dict) and "papers" in method_result:
                            raw_recommendations[category] = method_result["papers"]
                            logger.info(f"üìÑ {category}: Extracted {len(method_result['papers'])} papers from fallback method result")
                        else:
                            raw_recommendations[category] = []
                            logger.warning(f"‚ö†Ô∏è {category}: Fallback method result has unexpected structure: {type(method_result)}")

                    # Global deduplication already applied at algorithm level
                    recommendations = raw_recommendations
            else:
                # Traditional recommendation methods with global deduplication
                # Track used PMIDs across all methods to ensure diversity
                used_pmids = set()

                raw_method_results = {}

                # Generate papers in priority order to ensure diversity
                logger.info(f"üîÑ Starting global deduplication with empty used_pmids: {len(used_pmids)}")

                raw_method_results["papers_for_you"] = await self._generate_papers_for_you(user_profile, db, used_pmids)
                # Add papers from papers_for_you to used set
                if raw_method_results["papers_for_you"].get("papers"):
                    new_pmids = [p.get("pmid") for p in raw_method_results["papers_for_you"]["papers"] if p.get("pmid")]
                    used_pmids.update(new_pmids)
                    logger.info(f"üìÑ Papers for You added PMIDs: {new_pmids}, total used: {len(used_pmids)}")

                raw_method_results["trending_in_field"] = await self._generate_trending_in_field(user_profile, db, used_pmids)
                # Add papers from trending to used set
                if raw_method_results["trending_in_field"].get("papers"):
                    new_pmids = [p.get("pmid") for p in raw_method_results["trending_in_field"]["papers"] if p.get("pmid")]
                    used_pmids.update(new_pmids)
                    logger.info(f"üî• Trending added PMIDs: {new_pmids}, total used: {len(used_pmids)}")

                raw_method_results["cross_pollination"] = await self._generate_cross_pollination(user_profile, db, used_pmids)
                # Add papers from cross_pollination to used set
                if raw_method_results["cross_pollination"].get("papers"):
                    new_pmids = [p.get("pmid") for p in raw_method_results["cross_pollination"]["papers"] if p.get("pmid")]
                    used_pmids.update(new_pmids)
                    logger.info(f"üî¨ Cross-pollination added PMIDs: {new_pmids}, total used: {len(used_pmids)}")

                raw_method_results["citation_opportunities"] = await self._generate_citation_opportunities(user_profile, db, used_pmids)
                # Add papers from citation_opportunities to used set
                if raw_method_results["citation_opportunities"].get("papers"):
                    new_pmids = [p.get("pmid") for p in raw_method_results["citation_opportunities"]["papers"] if p.get("pmid")]
                    used_pmids.update(new_pmids)
                    logger.info(f"üí° Citation opportunities added PMIDs: {new_pmids}, total used: {len(used_pmids)}")

                # Extract papers arrays from method results (methods return {title, papers, updated})
                raw_recommendations = {}
                for category, method_result in raw_method_results.items():
                    if isinstance(method_result, dict) and "papers" in method_result:
                        raw_recommendations[category] = method_result["papers"]
                        logger.info(f"üìÑ {category}: Extracted {len(method_result['papers'])} papers from method result")
                    else:
                        raw_recommendations[category] = []
                        logger.warning(f"‚ö†Ô∏è {category}: Method result has unexpected structure: {type(method_result)}")

                # Global deduplication already applied at algorithm level
                recommendations = raw_recommendations

                # Debug: Check what we got from deduplication
                logger.info(f"üîç After deduplication, recommendations type: {type(recommendations)}")
                if isinstance(recommendations, dict):
                    logger.info(f"üîç Recommendations keys: {list(recommendations.keys())}")
                else:
                    logger.error(f"‚ùå Deduplication returned non-dict: {recommendations}")

            # üß† SEMANTIC ANALYSIS ENHANCEMENT
            logger.info("üß† Enhancing recommendations with semantic analysis...")

            try:
                # Enhance each category with semantic analysis
                if recommendations and isinstance(recommendations, dict):
                    # Now recommendations contains arrays directly, not nested in "papers" key
                    if recommendations.get("papers_for_you") and isinstance(recommendations["papers_for_you"], list):
                        enhanced_papers = await self._enhance_papers_with_semantic_analysis(
                            recommendations["papers_for_you"]
                        )
                        recommendations["papers_for_you"] = enhanced_papers

                    if recommendations.get("trending_in_field") and isinstance(recommendations["trending_in_field"], list):
                        enhanced_papers = await self._enhance_papers_with_semantic_analysis(
                            recommendations["trending_in_field"]
                        )
                        recommendations["trending_in_field"] = enhanced_papers

                    if recommendations.get("cross_pollination") and isinstance(recommendations["cross_pollination"], list):
                        enhanced_papers = await self._enhance_papers_with_semantic_analysis(
                            recommendations["cross_pollination"]
                        )
                        recommendations["cross_pollination"] = enhanced_papers

                    if recommendations.get("citation_opportunities") and isinstance(recommendations["citation_opportunities"], list):
                        enhanced_papers = await self._enhance_papers_with_semantic_analysis(
                            recommendations["citation_opportunities"]
                        )
                        recommendations["citation_opportunities"] = enhanced_papers
                else:
                    logger.warning(f"‚ö†Ô∏è Recommendations is not a valid dict: {type(recommendations)}")

                logger.info("‚úÖ Semantic analysis enhancement completed")

            except Exception as e:
                logger.error(f"‚ùå Error in semantic analysis enhancement: {e}")
                # Continue without semantic enhancement

            # Add weekly mix metadata
            now = datetime.now(timezone.utc)
            week_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
            week_start -= timedelta(days=week_start.weekday())  # Get Monday

            result = {
                "status": "success",
                "week_of": week_start.isoformat(),
                "user_id": user_id,
                "project_id": project_id,
                "recommendations": recommendations,
                "user_insights": {
                    "research_domains": user_profile.get("primary_domains", [])[:5],
                    "activity_level": user_profile.get("activity_level", "moderate"),
                    "discovery_preference": user_profile.get("discovery_preference", "balanced"),
                    "collaboration_tendency": user_profile.get("collaboration_score", 0.5)
                },
                "generated_at": now.isoformat(),
                "next_update": (week_start + timedelta(days=7)).isoformat()
            }

            # Cache the result
            self.recommendation_cache[cache_key] = {
                "data": result,
                "timestamp": datetime.now(timezone.utc)
            }

            return result

        except Exception as e:
            logger.error(f"Error generating weekly recommendations: {e}")
            return {"status": "error", "error": str(e)}
        finally:
            db.close()

    async def _get_available_papers_pool(self, user_id: str, project_id: Optional[str], db: Session) -> List[Dict[str, Any]]:
        """Get a pool of available papers for AI agents to analyze"""
        try:
            # Get recent papers from the database (last 2 years)
            cutoff_date = datetime.now(timezone.utc).year - 2

            query = db.query(Article).filter(
                Article.publication_year >= cutoff_date,
                Article.citation_count.isnot(None)
            ).order_by(desc(Article.citation_count)).limit(200)

            papers = query.all()

            # Convert to format expected by AI agents
            paper_pool = []
            for paper in papers:
                paper_dict = {
                    "pmid": paper.pmid,
                    "title": paper.title or "",
                    "abstract": paper.abstract or "",
                    "pub_year": paper.publication_year or datetime.now(timezone.utc).year,
                    "citation_count": paper.citation_count or 0,
                    "authors": paper.authors or [],
                    "journal": paper.journal or "",
                    "keywords": [],  # Not available in current schema
                    "mesh_terms": []  # Not available in current schema
                }
                paper_pool.append(paper_dict)

            logger.info(f"Retrieved {len(paper_pool)} papers for AI agent analysis")
            return paper_pool

        except Exception as e:
            logger.error(f"Error getting papers pool: {e}")
            return []

    async def _build_user_research_profile(self, user_id: str, project_id: Optional[str], db: Session) -> Dict[str, Any]:
        """Build comprehensive user research profile for personalized recommendations"""
        try:
            # Check behavior cache first
            cache_key = f"profile_{user_id}_{project_id or 'global'}"
            if cache_key in self.user_behavior_cache:
                cached_data = self.user_behavior_cache[cache_key]
                if datetime.now(timezone.utc) - cached_data["timestamp"] < self.behavior_cache_ttl:
                    return cached_data["data"]

            profile = {}

            # Resolve user - handle both email and UUID formats
            resolved_user_id = await self._resolve_user_id(user_id, db)
            logger.info(f"üîç Resolving user_id '{user_id}' to '{resolved_user_id}'")

            # PRIORITY 1: Get search history from frontend weekly mix automation
            search_history_domains = await self._get_search_history_domains(user_id)
            if search_history_domains:
                logger.info(f"üîç Found search history domains: {search_history_domains}")
                profile["primary_domains"] = search_history_domains
                profile["search_history_available"] = True
            else:
                logger.info(f"üîç No search history found for user {user_id}")
                profile["search_history_available"] = False

            # Get user's saved articles across all projects or specific project
            # Enhanced query with comprehensive debugging
            logger.info(f"üîç DEBUGGING USER DATA COLLECTION:")
            logger.info(f"üîç Original user_id: {user_id}")
            logger.info(f"üîç Resolved user_id: {resolved_user_id}")
            logger.info(f"üîç Project_id: {project_id}")

            # First, let's check if there are any ArticleCollection records at all
            total_article_collections = db.query(ArticleCollection).count()
            logger.info(f"üìä Total ArticleCollection records in database: {total_article_collections}")

            # Check if there are any collections for this user with multiple ID formats
            user_collections_query = db.query(Collection).filter(
                or_(
                    Collection.created_by == user_id,
                    Collection.created_by == resolved_user_id,
                    Collection.created_by.like(f"%{user_id.split('@')[0]}%") if '@' in user_id else False
                )
            )
            user_collections = user_collections_query.all()
            logger.info(f"üìö Found {len(user_collections)} collections for user")

            if user_collections:
                for col in user_collections[:5]:  # Log first 5 collections
                    logger.info(f"üìÅ Collection: '{col.collection_name}' (ID: {col.collection_id}, created_by: '{col.created_by}', project_id: '{col.project_id}')")
            else:
                # Debug: Check what collections exist in the database
                sample_collections = db.query(Collection).limit(5).all()
                logger.info(f"üìÅ Sample collections in database:")
                for col in sample_collections:
                    logger.info(f"üìÅ Sample: '{col.collection_name}' (created_by: '{col.created_by}')")

            # Check for articles in those collections
            if user_collections:
                collection_ids = [col.collection_id for col in user_collections]
                articles_in_collections = db.query(ArticleCollection).filter(
                    ArticleCollection.collection_id.in_(collection_ids)
                ).count()
                logger.info(f"üìÑ Total articles in user's collections: {articles_in_collections}")

            if project_id:
                # Query with project filter
                saved_articles_query = db.query(ArticleCollection).join(Collection).filter(
                    Collection.project_id == project_id,
                    or_(
                        Collection.created_by == user_id,
                        Collection.created_by == resolved_user_id,
                        ArticleCollection.added_by == user_id,
                        ArticleCollection.added_by == resolved_user_id
                    )
                ).order_by(desc(ArticleCollection.added_at)).limit(200)

                saved_articles = saved_articles_query.all()
                logger.info(f"üìä Project-specific query found {len(saved_articles)} articles")

                # If no articles found with project filter, try without project filter
                if len(saved_articles) == 0:
                    logger.info(f"üîç No articles found for project {project_id}, trying all projects...")
                    saved_articles_query = db.query(ArticleCollection).join(Collection).filter(
                        or_(
                            Collection.created_by == user_id,
                            Collection.created_by == resolved_user_id,
                            ArticleCollection.added_by == user_id,
                            ArticleCollection.added_by == resolved_user_id
                        )
                    ).order_by(desc(ArticleCollection.added_at)).limit(200)
                    saved_articles = saved_articles_query.all()
                    logger.info(f"üìä All-projects query found {len(saved_articles)} articles")
            else:
                # Query all projects
                saved_articles_query = db.query(ArticleCollection).join(Collection).filter(
                    or_(
                        Collection.created_by == user_id,
                        Collection.created_by == resolved_user_id,
                        ArticleCollection.added_by == user_id,
                        ArticleCollection.added_by == resolved_user_id
                    )
                ).order_by(desc(ArticleCollection.added_at)).limit(200)
                saved_articles = saved_articles_query.all()
                logger.info(f"üìä All-projects query found {len(saved_articles)} articles")

            # Debug: Log some sample articles if found
            if len(saved_articles) > 0:
                sample_article = saved_articles[0]
                logger.info(f"üìÑ Sample article: PMID={sample_article.article_pmid}, Title={sample_article.article_title[:50]}...")
                logger.info(f"üìÑ Sample article added_by: {sample_article.added_by}")
                logger.info(f"üìÑ Sample collection created_by: {sample_article.collection.created_by}")

            logger.info(f"üìä Final result: Found {len(saved_articles)} saved articles for user {user_id}")

            # If no saved articles, try to generate recommendations based on user activity
            if len(saved_articles) == 0:
                logger.info(f"üéØ No saved articles found, trying alternative recommendation approach for user {user_id}")

                # Try to get user's collections and projects for context
                try:
                    # Get user's collections
                    user_collections = db.query(Collection).filter(
                        or_(
                            Collection.created_by == user_id,
                            Collection.created_by == resolved_user_id
                        )
                    ).all()

                    logger.info(f"üìö Found {len(user_collections)} collections for user")

                    # Get user's projects
                    user_projects = db.query(Project).filter(
                        or_(
                            Project.owner_user_id == user_id,
                            Project.owner_user_id == resolved_user_id
                        )
                    ).all()

                    logger.info(f"üìÅ Found {len(user_projects)} projects for user")

                    # If user has collections, try to get articles directly from collections
                    if len(user_collections) > 0:
                        logger.info(f"‚úÖ User has collections, trying to get articles from collections directly")

                        # Get articles from user's collections using a different approach
                        collection_articles = []
                        for collection in user_collections:
                            articles_in_collection = db.query(ArticleCollection).filter(
                                ArticleCollection.collection_id == collection.collection_id
                            ).all()
                            collection_articles.extend(articles_in_collection)
                            logger.info(f"üìÑ Collection '{collection.collection_name}' has {len(articles_in_collection)} articles")

                        logger.info(f"üìä Total articles found in collections: {len(collection_articles)}")

                        if len(collection_articles) > 0:
                            # Use these articles as saved_articles for profile building
                            saved_articles = collection_articles
                            logger.info(f"‚úÖ Using {len(saved_articles)} articles from collections for profile building")
                        else:
                            # Extract research domains from collection names and descriptions
                            research_domains = []
                            for collection in user_collections:
                                collection_text = f"{collection.collection_name} {collection.description or ''}".lower()

                                # Detect research domains
                                if any(term in collection_text for term in ['kidney', 'renal', 'nephrology']):
                                    research_domains.append('nephrology')
                                if any(term in collection_text for term in ['diabetes', 'diabetic', 'glucose']):
                                    research_domains.append('diabetes')
                                if any(term in collection_text for term in ['cardiovascular', 'heart', 'cardiac']):
                                    research_domains.append('cardiovascular')
                                if any(term in collection_text for term in ['finerenone', 'mineralocorticoid', 'pharmacology']):
                                    research_domains.append('pharmacology')

                        # Remove duplicates
                        research_domains = list(set(research_domains))

                        if research_domains:
                            logger.info(f"üî¨ Detected research domains: {research_domains}")

                            # Create enhanced profile with detected domains
                            profile["primary_domains"] = research_domains
                            profile["topic_preferences"] = {domain: 0.8 for domain in research_domains}
                            profile["activity_level"] = "moderate"
                            profile["discovery_preference"] = "balanced"
                            profile["collaboration_score"] = 0.7
                            profile["research_velocity"] = "active"
                            profile["recency_bias"] = 0.8
                            profile["total_saved_papers"] = len(user_collections)
                            profile["is_enhanced_fallback"] = True

                            # Continue with recommendation generation instead of fallback
                            logger.info(f"‚úÖ Generated enhanced profile with {len(research_domains)} domains")
                        else:
                            logger.info(f"üìù No research domains detected, trying user subject area")
                            # Try to get user's subject area for recommendations
                            from database import User
                            user_record = db.query(User).filter(
                                or_(User.email == user_id, User.user_id == user_id)
                            ).first()

                            if user_record and user_record.subject_area:
                                logger.info(f"üéì Creating profile based on user's subject area: {user_record.subject_area}")
                                # Create a profile based on subject area
                                profile["primary_domains"] = [user_record.subject_area.lower()]
                                profile["topic_preferences"] = {user_record.subject_area.lower(): 0.9}
                                profile["activity_level"] = "new_user"
                                profile["discovery_preference"] = "exploratory"
                                profile["collaboration_score"] = 0.5
                                profile["research_velocity"] = "getting_started"
                                profile["recency_bias"] = 0.7
                                profile["total_saved_papers"] = 0
                                profile["is_subject_area_based"] = True
                                logger.info(f"‚úÖ Created subject-area-based profile for {user_record.subject_area}")
                            else:
                                return await self._generate_fallback_profile(user_id, db)
                    else:
                        logger.info(f"üìù No user collections found, trying user subject area")
                        # Try to get user's subject area for recommendations
                        from database import User
                        user_record = db.query(User).filter(
                            or_(User.email == user_id, User.user_id == user_id)
                        ).first()

                        if user_record and user_record.subject_area:
                            logger.info(f"üéì Creating profile based on user's subject area: {user_record.subject_area}")
                            # Create a profile based on subject area
                            profile["primary_domains"] = [user_record.subject_area.lower()]
                            profile["topic_preferences"] = {user_record.subject_area.lower(): 0.9}
                            profile["activity_level"] = "new_user"
                            profile["discovery_preference"] = "exploratory"
                            profile["collaboration_score"] = 0.5
                            profile["research_velocity"] = "getting_started"
                            profile["recency_bias"] = 0.7
                            profile["total_saved_papers"] = 0
                            profile["is_subject_area_based"] = True
                            logger.info(f"‚úÖ Created subject-area-based profile for {user_record.subject_area}")
                        else:
                            return await self._generate_fallback_profile(user_id, db)

                except Exception as e:
                    logger.error(f"‚ùå Error in alternative recommendation approach: {e}")
                    return await self._generate_fallback_profile(user_id, db)

            # Process saved articles if we have them (or if we created an enhanced profile)
            if len(saved_articles) > 0:
                # Extract research domains and topics from saved articles
                profile["primary_domains"] = await self._extract_research_domains(saved_articles, db)
                profile["topic_preferences"] = await self._analyze_topic_preferences(saved_articles, db)

                # Calculate user activity patterns
                profile["activity_level"] = self._calculate_activity_level(saved_articles)
                profile["discovery_preference"] = self._analyze_discovery_preference(saved_articles, db)
                profile["collaboration_score"] = self._calculate_collaboration_score(saved_articles, db)

                # Analyze temporal patterns
                profile["research_velocity"] = self._calculate_research_velocity(saved_articles)
                profile["recency_bias"] = self._calculate_recency_bias(saved_articles)

                # Collection analysis
                profile["collection_themes"] = await self._analyze_collection_themes(user_id, project_id, db)
                profile["organization_style"] = self._analyze_organization_style(user_id, project_id, db)

                # Store total saved papers
                profile["total_saved_papers"] = len(saved_articles)

            # If we have an enhanced fallback profile, we already set the necessary fields above
            elif profile.get("is_enhanced_fallback"):
                logger.info(f"‚úÖ Using enhanced fallback profile with domains: {profile.get('primary_domains', [])}")

                # Add collection analysis for enhanced fallback
                profile["collection_themes"] = await self._analyze_collection_themes(user_id, project_id, db)
                profile["organization_style"] = self._analyze_organization_style(user_id, project_id, db)

            # Cache the profile
            self.user_behavior_cache[cache_key] = {
                "data": profile,
                "timestamp": datetime.now(timezone.utc)
            }

            return profile

        except Exception as e:
            logger.error(f"Error building user research profile: {e}")
            return {}

    async def _resolve_user_id(self, user_id: str, db: Session) -> str:
        """Resolve user_id to handle both email and UUID formats"""
        try:
            from database import User

            # Try to find user by email first
            user = db.query(User).filter(User.email == user_id).first()
            if user:
                logger.info(f"‚úÖ Found user by email: {user_id} -> {user.user_id}")
                return user.user_id

            # Try to find user by user_id (UUID)
            user = db.query(User).filter(User.user_id == user_id).first()
            if user:
                logger.info(f"‚úÖ Found user by UUID: {user_id}")
                return user.user_id

            logger.warning(f"‚ö†Ô∏è User not found: {user_id}")
            return user_id

        except Exception as e:
            logger.error(f"‚ùå Error resolving user_id {user_id}: {e}")
            return user_id

    async def _get_search_history_domains(self, user_id: str) -> List[str]:
        """Get research domains from user's search history (frontend integration)"""
        try:
            import json
            import os

            # Try to read from localStorage simulation (file-based for backend)
            search_history_file = f"/tmp/search_history_{user_id.replace('@', '_').replace('.', '_')}.json"

            if os.path.exists(search_history_file):
                with open(search_history_file, 'r') as f:
                    search_data = json.load(f)

                # Extract domains from search queries
                domains = set()
                for entry in search_data.get('searches', []):
                    query = entry.get('query', '').lower()

                    # Domain detection from search queries
                    if any(term in query for term in ['kidney', 'renal', 'nephrology']):
                        domains.add('nephrology')
                    if any(term in query for term in ['diabetes', 'diabetic', 'glucose', 'insulin']):
                        domains.add('diabetes')
                    if any(term in query for term in ['cardiovascular', 'heart', 'cardiac', 'hypertension']):
                        domains.add('cardiovascular')
                    if any(term in query for term in ['finerenone', 'mineralocorticoid', 'pharmacology', 'drug']):
                        domains.add('pharmacology')
                    if any(term in query for term in ['machine learning', 'ai', 'artificial intelligence']):
                        domains.add('machine learning')
                    if any(term in query for term in ['cancer', 'oncology', 'tumor', 'chemotherapy']):
                        domains.add('oncology')
                    if any(term in query for term in ['neurology', 'brain', 'neurological', 'alzheimer']):
                        domains.add('neurology')

                return list(domains)

            # Fallback: Try to infer from user email domain or other signals
            if '@' in user_id:
                domain_part = user_id.split('@')[1].lower()
                if 'bayer' in domain_part or 'pharma' in domain_part:
                    return ['pharmacology', 'diabetes', 'cardiovascular']
                elif 'hospital' in domain_part or 'medical' in domain_part:
                    return ['nephrology', 'diabetes', 'cardiovascular']
                elif 'university' in domain_part or 'edu' in domain_part:
                    return ['machine learning', 'interdisciplinary studies']

            return []

        except Exception as e:
            logger.error(f"‚ùå Error getting search history domains: {e}")
            return []

    async def _generate_fallback_profile(self, user_id: str, db: Session) -> Dict[str, Any]:
        """Generate fallback profile for users with no saved articles"""
        try:
            from database import User

            # Get user info for basic profile
            user = db.query(User).filter(
                or_(User.email == user_id, User.user_id == user_id)
            ).first()

            fallback_profile = {
                "primary_domains": ["general research", "interdisciplinary studies"],
                "topic_preferences": {},
                "activity_level": "new_user",
                "discovery_preference": "exploratory",
                "collaboration_score": 0.5,
                "research_velocity": "getting_started",
                "recency_bias": 0.7,
                "collection_themes": [],
                "organization_style": "exploratory",
                "total_saved_papers": 0,
                "is_fallback": True
            }

            # Add user-specific info if available
            if user:
                if user.subject_area:
                    fallback_profile["primary_domains"] = [user.subject_area.lower()]
                if user.category:
                    fallback_profile["user_category"] = user.category
                if user.role:
                    fallback_profile["user_role"] = user.role

            logger.info(f"üéØ Generated fallback profile for user {user_id}")
            return fallback_profile

        except Exception as e:
            logger.error(f"‚ùå Error generating fallback profile: {e}")
            return {
                "primary_domains": ["general research"],
                "activity_level": "new_user",
                "is_fallback": True
            }

    async def _generate_fallback_recommendations(self, db: Session, category: str) -> Dict[str, Any]:
        """Generate intelligent recommendations for new users - Real papers, not mock data"""
        try:
            # Try multiple strategies to get diverse, real recommendations
            recent_papers = []

            # Strategy 1: High-impact recent papers (last 3 years, >20 citations) with valid metadata
            recent_papers = db.query(Article).filter(
                Article.publication_year >= datetime.now(timezone.utc).year - 3,
                Article.citation_count.isnot(None),
                Article.citation_count > 20,
                Article.title.isnot(None),
                Article.title != "",
                Article.title.notlike("Citation Article%"),
                Article.title.notlike("Reference Article%"),
                Article.title.notlike("Erratum%"),
                Article.title.notlike("Retraction%")
            ).order_by(desc(Article.citation_count)).limit(50).all()

            # Strategy 2: If not enough, try recent papers with any citations and valid metadata
            if len(recent_papers) < 15:
                additional_papers = db.query(Article).filter(
                    Article.publication_year >= datetime.now(timezone.utc).year - 3,
                    Article.citation_count.isnot(None),
                    Article.citation_count > 0,
                    Article.title.isnot(None),
                    Article.title != "",
                    ~Article.title.like("Citation Article%"),
                    ~Article.title.like("Reference Article%")
                ).order_by(desc(Article.citation_count)).limit(25).all()
                recent_papers.extend(additional_papers)

            # Strategy 3: If still not enough, get any recent papers with valid metadata
            if len(recent_papers) < 15:
                any_papers = db.query(Article).filter(
                    Article.publication_year >= datetime.now(timezone.utc).year - 5,
                    Article.title.isnot(None),
                    Article.title != "",
                    ~Article.title.like("Citation Article%"),
                    ~Article.title.like("Reference Article%")
                ).order_by(desc(Article.created_at)).limit(30).all()
                recent_papers.extend(any_papers)

            # Remove duplicates and ensure quality - increase limit for better variety
            seen_pmids = set()
            unique_papers = []
            for paper in recent_papers:
                if (paper.pmid and paper.pmid not in seen_pmids and
                    paper.title and paper.title.strip() and
                    len(paper.title.strip()) > 10):  # Ensure meaningful titles
                    unique_papers.append(paper)
                    seen_pmids.add(paper.pmid)
                if len(unique_papers) >= 30:  # Increased for better variety
                    break

            # Generate intelligent recommendations with category-specific scoring
            recommendations = []
            category_multipliers = {
                "papers_for_you": 0.85,
                "trending_in_field": 0.90,
                "cross_pollination": 0.80,
                "citation_opportunities": 0.75
            }

            base_multiplier = category_multipliers.get(category, 0.80)

            for i, paper in enumerate(unique_papers):
                # Calculate intelligent relevance score based on paper metrics
                citation_score = min(1.0, (paper.citation_count or 0) / 100)  # Normalize citations
                recency_score = max(0.3, 1.0 - ((datetime.now(timezone.utc).year - (paper.publication_year or 2020)) / 10))
                relevance_score = (citation_score * 0.6 + recency_score * 0.4) * base_multiplier

                # Generate category-specific reasons
                reasons = {
                    "papers_for_you": f"High-impact research with {paper.citation_count or 0} citations",
                    "trending_in_field": f"Trending paper from {paper.publication_year or 'recent'} with growing citations",
                    "cross_pollination": f"Interdisciplinary research bridging multiple domains",
                    "citation_opportunities": f"Influential work in {paper.journal or 'top journal'}"
                }

                recommendations.append({
                    "pmid": paper.pmid,
                    "title": paper.title,
                    "authors": paper.authors[:3] if paper.authors else ["Research Team"],
                    "journal": paper.journal or "Academic Journal",
                    "year": paper.publication_year or datetime.now(timezone.utc).year,
                    "citation_count": paper.citation_count or 0,
                    "relevance_score": round(relevance_score, 2),
                    "reason": reasons.get(category, "High-quality research paper"),
                    "category": category,
                    "is_fallback": False,  # These are real papers, not fallback
                    "quality_score": citation_score,
                    "recency_score": recency_score
                })

            logger.info(f"üéØ Generated {len(recommendations)} fallback recommendations for category {category}")

            return {
                "recommendations": recommendations,
                "total": len(recommendations),
                "category": category,
                "updated": datetime.now(timezone.utc).isoformat(),
                "refresh_reason": "Welcome! Here are some popular papers to get you started"
            }

        except Exception as e:
            logger.error(f"‚ùå Error generating fallback recommendations: {e}")
            return {
                "recommendations": [],
                "total": 0,
                "category": category,
                "error": str(e)
            }
    
    async def _generate_papers_for_you(self, user_profile: Dict, db: Session, used_pmids: set = None) -> Dict[str, Any]:
        """Generate personalized 'Papers for You' recommendations"""
        try:
            recommendations = []
            primary_domains = user_profile.get("primary_domains", [])
            topic_preferences = user_profile.get("topic_preferences", {})

            logger.info(f"üí° Papers-for-You: User domains = {primary_domains}")
            logger.info(f"üí° Papers-for-You: User profile fallback = {user_profile.get('is_fallback', False)}")

            # NEVER use fallback - always generate personalized recommendations
            # If no primary domains, use intelligent defaults based on user context
            if not primary_domains or primary_domains == ["general research"]:
                # PAPERS FOR YOU: Focus on diverse, high-quality recent research
                primary_domains = ["medicine", "biology", "neuroscience", "genetics"]
                logger.info(f"üí° Papers-for-You: Using personalized default domains: {primary_domains}")

            # Enhanced domain keyword mapping for better personalization
            domain_keywords = {
                "nephrology": ["kidney", "renal", "dialysis", "creatinine", "nephrology"],
                "diabetes": ["diabetic", "glucose", "insulin", "glycemic", "diabetes"],
                "cardiovascular": ["heart", "cardiac", "hypertension", "blood pressure", "cardiovascular"],
                "pharmacology": ["drug", "medication", "therapeutic", "treatment", "pharmacology"],
                "machine learning": ["AI", "artificial intelligence", "algorithm", "neural", "machine learning"],
                "oncology": ["cancer", "tumor", "chemotherapy", "malignant", "oncology"],
                "neurology": ["brain", "neurological", "alzheimer", "neurology", "cognitive"],
                "medicine": ["medical", "clinical", "patient", "treatment", "therapy"],
                "biology": ["biological", "molecular", "cellular", "genetic", "protein"],
                "chemistry": ["chemical", "compound", "synthesis", "molecular", "reaction"]
            }

            # Track seen PMIDs to avoid duplicates within this method
            seen_pmids = set()
            # Also avoid PMIDs already used by other methods
            if used_pmids is None:
                used_pmids = set()
            global_used_pmids = used_pmids.copy()

            # Get papers based on user's research domains with multiple strategies
            for domain in primary_domains[:3]:  # Top 3 domains
                keywords = domain_keywords.get(domain.lower(), [domain])
                logger.info(f"üí° Processing domain '{domain}' with keywords: {keywords}")

                # Strategy 1: PERSONALIZED - High-quality established papers (last 4 years)
                for keyword in keywords[:3]:
                    domain_papers = db.query(Article).filter(
                        or_(
                            Article.title.ilike(f'%{keyword}%'),
                            Article.abstract.ilike(f'%{keyword}%')
                        ),
                        Article.publication_year >= datetime.now(timezone.utc).year - 4,  # Slightly older for quality
                        Article.citation_count > 20,  # Higher quality threshold
                        Article.title.isnot(None),
                        Article.title != "",
                        ~Article.title.like("Citation Article%"),
                        ~Article.title.like("Reference Article%")
                    ).order_by(desc(Article.citation_count)).limit(8).all()

                    if domain_papers:
                        logger.info(f"üí° Strategy 1 ({keyword}): Found {len(domain_papers)} recent papers")
                        break

                # Strategy 2: Expand time range if needed
                if not domain_papers:
                    for keyword in keywords[:3]:
                        domain_papers = db.query(Article).filter(
                            or_(
                                Article.title.ilike(f'%{keyword}%'),
                                Article.abstract.ilike(f'%{keyword}%')
                            ),
                            Article.publication_year >= datetime.now(timezone.utc).year - 5,
                            Article.title.isnot(None),
                            Article.title != ""
                        ).order_by(desc(Article.citation_count)).limit(6).all()

                        if domain_papers:
                            logger.info(f"üí° Strategy 2 ({keyword}): Found {len(domain_papers)} papers")
                            break

                # Strategy 3: Get any papers in domain
                if not domain_papers:
                    for keyword in keywords[:2]:
                        domain_papers = db.query(Article).filter(
                            or_(
                                Article.title.ilike(f'%{keyword}%'),
                                Article.abstract.ilike(f'%{keyword}%')
                            ),
                            Article.title.isnot(None),
                            Article.title != ""
                        ).order_by(desc(Article.citation_count)).limit(4).all()

                        if domain_papers:
                            logger.info(f"üí° Strategy 3 ({keyword}): Found {len(domain_papers)} papers")
                            break

                # Strategy 4: If still no results, get any quality papers and personalize the reason
                if not domain_papers:
                    domain_papers = db.query(Article).filter(
                        Article.title.isnot(None),
                        Article.title != "",
                        Article.citation_count > 0
                    ).order_by(desc(Article.citation_count)).limit(3).all()
                    logger.info(f"üí° Strategy 4 (quality papers for {domain}): Found {len(domain_papers)} papers")

                # Process found papers (with deduplication)
                for paper in domain_papers:
                    # Skip if we've already seen this paper OR if it's used by other methods
                    if paper.pmid in seen_pmids or paper.pmid in global_used_pmids:
                        continue

                    seen_pmids.add(paper.pmid)
                    relevance_score = self._calculate_personalized_relevance(paper, user_profile)

                    # Generate personalized reason based on user's search history
                    reason = self._generate_personalized_reason(domain, user_profile)

                    recommendations.append({
                        "pmid": paper.pmid,
                        "title": paper.title,
                        "authors": paper.authors[:3] if paper.authors else [],
                        "journal": paper.journal,
                        "year": paper.publication_year,
                        "citation_count": paper.citation_count or 0,
                        "relevance_score": relevance_score,
                        "reason": reason,
                        "category": "papers_for_you",
                        "is_fallback": False,  # NEVER mark as fallback
                        "spotify_style": {
                            "cover_color": self._generate_cover_color(domain),
                            "subtitle": f"Because you research {domain}",
                            "play_count": paper.citation_count or 0
                        }
                    })

            # Sort by relevance and return top recommendations
            recommendations.sort(key=lambda x: x["relevance_score"], reverse=True)

            # Ensure we have at least some recommendations
            if len(recommendations) < 3:
                logger.info(f"üí° Only {len(recommendations)} recommendations found, adding fallback papers")
                fallback_papers = db.query(Article).filter(
                    Article.title.isnot(None),
                    Article.title != ""
                ).order_by(desc(Article.citation_count)).limit(5).all()

                for paper in fallback_papers:
                    if paper.pmid not in [r["pmid"] for r in recommendations] and paper.pmid not in global_used_pmids:
                        recommendations.append({
                            "pmid": paper.pmid,
                            "title": paper.title,
                            "authors": paper.authors[:3] if paper.authors else [],
                            "journal": paper.journal,
                            "year": paper.publication_year,
                            "citation_count": paper.citation_count or 0,
                            "relevance_score": 0.6,
                            "reason": "High-impact research in your field",
                            "category": "papers_for_you",
                            "is_fallback": False,  # Still not marked as fallback
                            "spotify_style": {
                                "cover_color": "#3498db",
                                "subtitle": "Popular research",
                                "play_count": paper.citation_count or 0
                            }
                        })
                        if len(recommendations) >= 8:
                            break

            logger.info(f"üí° Generated {len(recommendations)} personalized recommendations")

            return {
                "title": "Papers for You",
                "description": "Your personalized research feed",
                "papers": recommendations[:20],  # Spotify-style abundance of 20
                "updated": datetime.now(timezone.utc).isoformat(),
                "refresh_reason": "Based on your research interests and search history"
            }

        except Exception as e:
            logger.error(f"Error generating Papers for You: {e}")
            return {"title": "Papers for You", "papers": [], "error": str(e)}

    def _generate_personalized_reason(self, domain: str, user_profile: Dict) -> str:
        """Generate personalized reason based on user's profile and search history"""
        try:
            search_history_available = user_profile.get("search_history_available", False)

            if search_history_available:
                reasons = [
                    f"Based on your recent searches in {domain}",
                    f"Matches your research focus on {domain}",
                    f"Recommended because you've been exploring {domain}",
                    f"Aligns with your {domain} research interests",
                    f"Popular in {domain} - your area of expertise"
                ]
            else:
                reasons = [
                    f"Trending research in {domain}",
                    f"High-impact work in {domain}",
                    f"Recent advances in {domain}",
                    f"Important findings in {domain}",
                    f"Breakthrough research in {domain}"
                ]

            import random
            return random.choice(reasons)

        except Exception as e:
            return f"Recommended for your {domain} research"
    
    async def _generate_trending_in_field(self, user_profile: Dict, db: Session, used_pmids: set = None) -> Dict[str, Any]:
        """Generate 'Trending in Your Field' recommendations"""
        try:
            recommendations = []
            primary_domains = user_profile.get("primary_domains", [])

            logger.info(f"üî• Trending: User domains = {primary_domains}")

            # If no primary domains, use fallback domains
            if not primary_domains or primary_domains == ["general research"]:
                # TRENDING: Focus on hot topics and emerging fields
                primary_domains = ["artificial intelligence", "immunology", "cancer research", "climate science"]
                logger.info(f"üî• Trending: Using hot topic domains: {primary_domains}")

            # Enhanced domain keyword mapping for better search
            domain_keywords = {
                "nephrology": ["kidney", "renal", "dialysis", "creatinine", "nephrology"],
                "diabetes": ["diabetic", "glucose", "insulin", "glycemic", "diabetes"],
                "cardiovascular": ["heart", "cardiac", "hypertension", "blood pressure", "cardiovascular"],
                "pharmacology": ["drug", "medication", "therapeutic", "treatment", "pharmacology"],
                "machine learning": ["AI", "artificial intelligence", "algorithm", "neural", "machine learning"],
                "oncology": ["cancer", "tumor", "chemotherapy", "malignant", "oncology"],
                "neurology": ["brain", "neurological", "alzheimer", "neurology", "cognitive"],
                "medicine": ["medical", "clinical", "patient", "treatment", "therapy"],
                "biology": ["biological", "molecular", "cellular", "genetic", "protein"],
                "chemistry": ["chemical", "compound", "synthesis", "molecular", "reaction"]
            }

            # Track seen PMIDs to avoid duplicates within this method
            seen_pmids = set()
            # Also avoid PMIDs already used by other methods
            if used_pmids is None:
                used_pmids = set()
            global_used_pmids = used_pmids.copy()

            logger.info(f"üî• Trending: Starting with {len(global_used_pmids)} used PMIDs: {list(global_used_pmids)[:5]}{'...' if len(global_used_pmids) > 5 else ''}")

            # Get trending papers in user's research fields with multiple strategies
            for domain in primary_domains[:3]:  # Increased from 2 to 3
                trending_papers = []
                keywords = domain_keywords.get(domain.lower(), [domain])

                logger.info(f"üî• Processing domain '{domain}' with keywords: {keywords}")

                # Strategy 1: TRENDING - Very recent papers (last 2 years) with good citation velocity
                for keyword in keywords[:3]:  # Try top 3 keywords
                    trending_papers = db.query(Article).filter(
                        or_(
                            Article.title.ilike(f'%{keyword}%'),
                            Article.abstract.ilike(f'%{keyword}%')
                        ),
                        Article.publication_year >= datetime.now(timezone.utc).year - 2,  # More recent for trending
                        Article.citation_count > 5,  # Lower threshold but more recent
                        Article.title.isnot(None),
                        Article.title != "",
                        ~Article.title.like("Citation Article%"),
                        ~Article.title.like("Reference Article%")
                    ).order_by(desc(Article.citation_count)).limit(8).all()

                    if trending_papers:
                        logger.info(f"üî• Strategy 1 ({keyword}): Found {len(trending_papers)} recent trending papers")
                        break

                # Strategy 2: Expand time range if needed (last 5 years, lower citation threshold)
                if not trending_papers:
                    for keyword in keywords[:3]:
                        trending_papers = db.query(Article).filter(
                            or_(
                                Article.title.ilike(f'%{keyword}%'),
                                Article.abstract.ilike(f'%{keyword}%')
                            ),
                            Article.publication_year >= datetime.now(timezone.utc).year - 5,
                            Article.citation_count > 5,  # Lower threshold
                            Article.title.isnot(None),
                            Article.title != ""
                        ).order_by(desc(Article.citation_count)).limit(8).all()

                        if trending_papers:
                            logger.info(f"üî• Strategy 2 ({keyword}): Found {len(trending_papers)} trending papers")
                            break

                # Strategy 3: Get any papers in the domain (no date restriction)
                if not trending_papers:
                    for keyword in keywords[:3]:
                        trending_papers = db.query(Article).filter(
                            or_(
                                Article.title.ilike(f'%{keyword}%'),
                                Article.abstract.ilike(f'%{keyword}%')
                            ),
                            Article.title.isnot(None),
                            Article.title != ""
                        ).order_by(desc(Article.citation_count)).limit(5).all()

                        if trending_papers:
                            logger.info(f"üî• Strategy 3 ({keyword}): Found {len(trending_papers)} domain papers")
                            break

                # Strategy 4: If still no results, get any high-citation papers and mark as trending
                if not trending_papers:
                    trending_papers = db.query(Article).filter(
                        Article.title.isnot(None),
                        Article.title != "",
                        Article.citation_count > 0
                    ).order_by(desc(Article.citation_count)).limit(3).all()
                    logger.info(f"üî• Strategy 4 (high-citation papers): Found {len(trending_papers)} papers")

                for paper in trending_papers:
                    # Skip if we've already seen this paper OR if it's used by other methods
                    if paper.pmid in seen_pmids or paper.pmid in global_used_pmids:
                        continue

                    seen_pmids.add(paper.pmid)

                    # Calculate trending score based on citations per month since publication
                    months_since_pub = max(1, (datetime.now(timezone.utc).year - (paper.publication_year or datetime.now(timezone.utc).year)) * 12)
                    trending_score = (paper.citation_count or 0) / months_since_pub

                    recommendations.append({
                        "pmid": paper.pmid,
                        "title": paper.title,
                        "authors": paper.authors[:3] if paper.authors else [],
                        "journal": paper.journal,
                        "year": paper.publication_year,
                        "citation_count": paper.citation_count or 0,
                        "trending_score": trending_score,
                        "reason": f"Trending in {domain}",
                        "category": "trending_in_field",
                        "spotify_style": {
                            "cover_color": "#ff6b35",  # Orange for trending
                            "subtitle": f"Hot in {domain}",
                            "trend_indicator": "üî•"
                        }
                    })

            # Sort by trending score
            recommendations.sort(key=lambda x: x["trending_score"], reverse=True)

            logger.info(f"üî• Trending: Generated {len(recommendations)} recommendations with PMIDs: {[r['pmid'] for r in recommendations[:5]]}{'...' if len(recommendations) > 5 else ''}")

            # Ensure we have at least some recommendations
            if len(recommendations) < 3:
                logger.info(f"üî• Only {len(recommendations)} trending recommendations found, adding fallback papers")
                fallback_papers = db.query(Article).filter(
                    Article.title.isnot(None),
                    Article.title != ""
                ).order_by(desc(Article.citation_count)).limit(5).all()

                for paper in fallback_papers:
                    if paper.pmid not in [r["pmid"] for r in recommendations] and paper.pmid not in global_used_pmids:
                        months_since_pub = max(1, (datetime.now(timezone.utc).year - (paper.publication_year or datetime.now(timezone.utc).year)) * 12)
                        trending_score = (paper.citation_count or 0) / months_since_pub

                        recommendations.append({
                            "pmid": paper.pmid,
                            "title": paper.title,
                            "authors": paper.authors[:3] if paper.authors else [],
                            "journal": paper.journal,
                            "year": paper.publication_year,
                            "citation_count": paper.citation_count or 0,
                            "trending_score": trending_score,
                            "reason": "High-impact trending research",
                            "category": "trending_in_field",
                            "spotify_style": {
                                "cover_color": "#ff6b35",
                                "subtitle": "Popular research",
                                "trend_indicator": "üî•"
                            }
                        })
                        if len(recommendations) >= 6:
                            break

            return {
                "title": "Trending in Your Field",
                "description": "Hot topics and emerging research in your areas",
                "papers": recommendations[:10],
                "updated": datetime.now(timezone.utc).isoformat(),
                "refresh_reason": "Based on recent citation activity in your research domains"
            }

        except Exception as e:
            logger.error(f"Error generating Trending in Field: {e}")
            return {"title": "Trending in Your Field", "papers": [], "error": str(e)}
    
    async def _generate_cross_pollination(self, user_profile: Dict, db: Session, used_pmids: set = None) -> Dict[str, Any]:
        """Generate 'Cross-pollination' interdisciplinary discovery recommendations"""
        try:
            recommendations = []
            primary_domains = user_profile.get("primary_domains", [])

            logger.info(f"üî¨ Cross-pollination: User domains = {primary_domains}")

            # Enhanced interdisciplinary connection mappings with medical focus
            cross_domain_map = {
                "nephrology": ["diabetes", "cardiovascular", "pharmacology", "machine learning", "data science"],
                "diabetes": ["nephrology", "cardiovascular", "pharmacology", "endocrinology", "nutrition"],
                "cardiovascular": ["nephrology", "diabetes", "pharmacology", "cardiology", "hypertension"],
                "pharmacology": ["nephrology", "diabetes", "cardiovascular", "chemistry", "drug discovery"],
                "machine learning": ["medicine", "biology", "pharmacology", "data science", "bioinformatics"],
                "oncology": ["immunology", "pharmacology", "genetics", "machine learning", "pathology"],
                "neurology": ["psychology", "pharmacology", "machine learning", "genetics", "psychiatry"],
                "biology": ["computer science", "engineering", "mathematics", "physics", "chemistry"],
                "medicine": ["engineering", "data science", "psychology", "biology", "pharmacology"],
                "physics": ["computer science", "engineering", "mathematics", "chemistry", "biology"],
                "chemistry": ["biology", "physics", "materials science", "engineering", "pharmacology"],
                "psychology": ["neuroscience", "computer science", "medicine", "sociology", "psychiatry"],
                "engineering": ["biology", "medicine", "computer science", "physics", "bioengineering"],
                "neuroscience": ["psychology", "computer science", "medicine", "biology", "pharmacology"]
            }

            # If no primary domains, use fallback domains for cross-pollination
            if not primary_domains or primary_domains == ["general research"]:
                # CROSS-POLLINATION: Focus on interdisciplinary connections
                primary_domains = ["bioengineering", "computational biology", "digital health"]
                logger.info(f"üî¨ Cross-pollination: Using interdisciplinary domains: {primary_domains}")

            # Track seen PMIDs to avoid duplicates within this method
            seen_pmids = set()
            # Also avoid PMIDs already used by other methods
            if used_pmids is None:
                used_pmids = set()
            global_used_pmids = used_pmids.copy()

            # Find papers at the intersection of user's domains and adjacent fields
            for primary_domain in primary_domains[:3]:  # Increased from 2 to 3
                adjacent_fields = cross_domain_map.get(primary_domain.lower(), [])
                logger.info(f"üî¨ Primary domain '{primary_domain}' -> Adjacent fields: {adjacent_fields[:3]}")

                for adjacent_field in adjacent_fields[:3]:
                    # Find papers that mention both domains with flexible search strategies
                    cross_papers = []

                    # Strategy 1: Exact domain matches in title/abstract
                    cross_papers = db.query(Article).filter(
                        and_(
                            or_(
                                Article.title.ilike(f'%{primary_domain}%'),
                                Article.abstract.ilike(f'%{primary_domain}%')
                            ),
                            or_(
                                Article.title.ilike(f'%{adjacent_field}%'),
                                Article.abstract.ilike(f'%{adjacent_field}%')
                            )
                        ),
                        Article.title.isnot(None),
                        Article.title != "",
                        ~Article.title.like("Citation Article%"),
                        ~Article.title.like("Reference Article%")
                    ).order_by(desc(Article.citation_count)).limit(8).all()

                    logger.info(f"üî¨ Strategy 1 ({primary_domain} + {adjacent_field}): Found {len(cross_papers)} papers")

                    # Strategy 2: If no results, try broader keyword matching
                    if not cross_papers:
                        # Define broader keywords for each domain
                        domain_keywords = {
                            "nephrology": ["kidney", "renal", "dialysis", "creatinine"],
                            "diabetes": ["diabetic", "glucose", "insulin", "glycemic"],
                            "cardiovascular": ["heart", "cardiac", "hypertension", "blood pressure"],
                            "pharmacology": ["drug", "medication", "therapeutic", "treatment"],
                            "machine learning": ["AI", "artificial intelligence", "algorithm", "neural"],
                            "oncology": ["cancer", "tumor", "chemotherapy", "malignant"]
                        }

                        primary_keywords = domain_keywords.get(primary_domain.lower(), [primary_domain])
                        adjacent_keywords = domain_keywords.get(adjacent_field.lower(), [adjacent_field])

                        for p_keyword in primary_keywords[:2]:
                            for a_keyword in adjacent_keywords[:2]:
                                cross_papers = db.query(Article).filter(
                                    and_(
                                        or_(
                                            Article.title.ilike(f'%{p_keyword}%'),
                                            Article.abstract.ilike(f'%{p_keyword}%')
                                        ),
                                        or_(
                                            Article.title.ilike(f'%{a_keyword}%'),
                                            Article.abstract.ilike(f'%{a_keyword}%')
                                        )
                                    ),
                                    Article.title.isnot(None),
                                    Article.title != ""
                                ).order_by(desc(Article.citation_count)).limit(5).all()

                                if cross_papers:
                                    logger.info(f"üî¨ Strategy 2 ({p_keyword} + {a_keyword}): Found {len(cross_papers)} papers")
                                    break
                            if cross_papers:
                                break

                    # Strategy 3: If still no results, get papers from each domain separately
                    if not cross_papers:
                        # Get papers from primary domain
                        primary_papers = db.query(Article).filter(
                            or_(
                                Article.title.ilike(f'%{primary_domain}%'),
                                Article.abstract.ilike(f'%{primary_domain}%')
                            ),
                            Article.title.isnot(None),
                            Article.title != ""
                        ).order_by(desc(Article.citation_count)).limit(3).all()

                        # Get papers from adjacent field
                        adjacent_papers = db.query(Article).filter(
                            or_(
                                Article.title.ilike(f'%{adjacent_field}%'),
                                Article.abstract.ilike(f'%{adjacent_field}%')
                            ),
                            Article.title.isnot(None),
                            Article.title != ""
                        ).order_by(desc(Article.citation_count)).limit(3).all()

                        cross_papers = primary_papers + adjacent_papers
                        logger.info(f"üî¨ Strategy 3 (separate domains): Found {len(cross_papers)} papers")

                    # Strategy 4: If still no results, get any high-quality papers and mark as cross-domain
                    if not cross_papers:
                        cross_papers = db.query(Article).filter(
                            Article.title.isnot(None),
                            Article.title != "",
                            Article.citation_count > 0
                        ).order_by(desc(Article.citation_count)).limit(2).all()
                        logger.info(f"üî¨ Strategy 4 (high-quality papers): Found {len(cross_papers)} papers")

                    for paper in cross_papers:
                        # Skip if we've already seen this paper OR if it's used by other methods
                        if paper.pmid in seen_pmids or paper.pmid in global_used_pmids:
                            continue

                        seen_pmids.add(paper.pmid)

                        cross_pollination_score = self._calculate_interdisciplinary_score(
                            paper, primary_domain, adjacent_field
                        )

                        recommendations.append({
                            "pmid": paper.pmid,
                            "title": paper.title,
                            "authors": paper.authors[:3] if paper.authors else [],
                            "journal": paper.journal,
                            "year": paper.publication_year,
                            "citation_count": paper.citation_count or 0,
                            "cross_pollination_score": cross_pollination_score,
                            "reason": f"Bridges {primary_domain} and {adjacent_field}",
                            "category": "cross_pollination",
                            "spotify_style": {
                                "cover_color": "#9b59b6",  # Purple for interdisciplinary
                                "subtitle": f"{primary_domain} √ó {adjacent_field}",
                                "discovery_badge": "üî¨"
                            }
                        })

            # Sort by cross-pollination score
            recommendations.sort(key=lambda x: x["cross_pollination_score"], reverse=True)

            # Ensure we have at least some recommendations
            if len(recommendations) < 3:
                logger.info(f"üî¨ Only {len(recommendations)} cross-pollination recommendations found, adding fallback papers")
                fallback_papers = db.query(Article).filter(
                    Article.title.isnot(None),
                    Article.title != ""
                ).order_by(desc(Article.citation_count)).limit(5).all()

                for paper in fallback_papers:
                    if paper.pmid not in [r["pmid"] for r in recommendations] and paper.pmid not in global_used_pmids:
                        cross_pollination_score = 0.6  # Default score for fallback

                        recommendations.append({
                            "pmid": paper.pmid,
                            "title": paper.title,
                            "authors": paper.authors[:3] if paper.authors else [],
                            "journal": paper.journal,
                            "year": paper.publication_year,
                            "citation_count": paper.citation_count or 0,
                            "cross_pollination_score": cross_pollination_score,
                            "reason": "Interdisciplinary research opportunity",
                            "category": "cross_pollination",
                            "spotify_style": {
                                "cover_color": "#9b59b6",
                                "subtitle": "Cross-domain insights",
                                "discovery_badge": "üî¨"
                            }
                        })
                        if len(recommendations) >= 5:
                            break

            return {
                "title": "Cross-pollination",
                "description": "Interdisciplinary discoveries at the intersection of your research",
                "papers": recommendations[:8],
                "updated": datetime.now(timezone.utc).isoformat(),
                "refresh_reason": "Exploring connections between your research domains and adjacent fields"
            }

        except Exception as e:
            logger.error(f"Error generating Cross-pollination: {e}")
            return {"title": "Cross-pollination", "papers": [], "error": str(e)}
    
    async def _generate_citation_opportunities(self, user_profile: Dict, db: Session, used_pmids: set = None) -> Dict[str, Any]:
        """Generate 'Citation Opportunities' - papers that could cite user's work"""
        try:
            recommendations = []
            primary_domains = user_profile.get("primary_domains", [])
            topic_preferences = user_profile.get("topic_preferences", {})

            # Track seen PMIDs to avoid duplicates within this method
            seen_pmids = set()
            # Also avoid PMIDs already used by other methods
            if used_pmids is None:
                used_pmids = set()
            global_used_pmids = used_pmids.copy()

            logger.info(f"üí° Citation Opportunities: Starting with {len(global_used_pmids)} used PMIDs: {list(global_used_pmids)[:5]}{'...' if len(global_used_pmids) > 5 else ''}")

            # Find recent papers in user's field that might benefit from citing their work
            for domain in primary_domains[:3]:
                # Get recent papers with flexible date range
                recent_papers = []

                # Try different time ranges to find suitable papers with valid metadata
                for years_back in [1, 2, 3, 5]:
                    recent_papers = db.query(Article).filter(
                        or_(
                            Article.title.ilike(f'%{domain}%'),
                            Article.abstract.ilike(f'%{domain}%')
                        ),
                        Article.publication_year >= datetime.now(timezone.utc).year - years_back,
                        Article.citation_count < 100,  # Papers that could use more citations
                        Article.title.isnot(None),
                        Article.title != "",
                        ~Article.title.like("Citation Article%"),
                        ~Article.title.like("Reference Article%")
                    ).order_by(desc(Article.publication_year), desc(Article.citation_count)).limit(15).all()

                    if recent_papers:
                        break

                # If still no results, get any papers in the domain with moderate citations and valid metadata
                if not recent_papers:
                    recent_papers = db.query(Article).filter(
                        or_(
                            Article.title.ilike(f'%{domain}%'),
                            Article.abstract.ilike(f'%{domain}%')
                        ),
                        Article.citation_count < 100,
                        Article.title.isnot(None),
                        Article.title != "",
                        ~Article.title.like("Citation Article%"),
                        ~Article.title.like("Reference Article%")
                    ).order_by(desc(Article.publication_year), desc(Article.citation_count)).limit(15).all()

                for paper in recent_papers:
                    # Skip if we've already seen this paper OR if it's used by other methods
                    if paper.pmid in seen_pmids or paper.pmid in global_used_pmids:
                        logger.info(f"üí° Citation Opportunities: Skipping duplicate PMID {paper.pmid} (seen: {paper.pmid in seen_pmids}, global: {paper.pmid in global_used_pmids})")
                        continue

                    seen_pmids.add(paper.pmid)
                    citation_opportunity_score = self._calculate_citation_opportunity_score(
                        paper, user_profile
                    )

                    recommendations.append({
                        "pmid": paper.pmid,
                        "title": paper.title,
                        "authors": paper.authors[:3] if paper.authors else [],
                        "journal": paper.journal,
                        "year": paper.publication_year,
                        "citation_count": paper.citation_count or 0,
                        "opportunity_score": citation_opportunity_score,
                        "reason": f"Recent work in {domain} that could benefit from your research",
                        "category": "citation_opportunities",
                        "spotify_style": {
                            "cover_color": "#27ae60",  # Green for opportunities
                            "subtitle": f"Opportunity in {domain}",
                            "opportunity_badge": "üí°"
                        }
                    })

            # Sort by opportunity score
            recommendations.sort(key=lambda x: x["opportunity_score"], reverse=True)

            logger.info(f"üí° Citation Opportunities: Generated {len(recommendations)} recommendations with PMIDs: {[r['pmid'] for r in recommendations[:5]]}{'...' if len(recommendations) > 5 else ''}")

            return {
                "title": "Citation Opportunities",
                "description": "Recent papers that could cite your work",
                "papers": recommendations[:8],
                "updated": datetime.now(timezone.utc).isoformat(),
                "refresh_reason": "Based on recent publications in your research areas"
            }

        except Exception as e:
            logger.error(f"Error generating Citation Opportunities: {e}")
            return {"title": "Citation Opportunities", "papers": [], "error": str(e)}
    
    # Helper methods for user behavior analysis
    async def _extract_research_domains(self, saved_articles: List, db: Session) -> List[str]:
        """Extract primary research domains from user's saved articles"""
        try:
            domain_keywords = {
                "machine learning": ["machine learning", "deep learning", "neural network", "ai", "artificial intelligence"],
                "biology": ["biology", "biological", "organism", "cell", "gene", "protein", "dna"],
                "medicine": ["medical", "clinical", "patient", "treatment", "therapy", "disease", "diagnosis"],
                "physics": ["physics", "quantum", "particle", "energy", "matter", "force"],
                "chemistry": ["chemistry", "chemical", "molecule", "compound", "reaction", "synthesis"],
                "neuroscience": ["neuroscience", "brain", "neural", "cognitive", "neurological"],
                "engineering": ["engineering", "design", "system", "optimization", "control"],
                "psychology": ["psychology", "behavior", "cognitive", "mental", "psychological"]
            }

            domain_scores = defaultdict(int)

            for article_collection in saved_articles:
                title = (article_collection.article_title or "").lower()

                for domain, keywords in domain_keywords.items():
                    for keyword in keywords:
                        if keyword in title:
                            domain_scores[domain] += 1

            # Return top domains
            return [domain for domain, score in sorted(domain_scores.items(), key=lambda x: x[1], reverse=True)][:5]

        except Exception as e:
            logger.error(f"Error extracting research domains: {e}")
            return []
    
    def _calculate_personalized_relevance(self, paper: Article, user_profile: Dict) -> float:
        """Calculate personalized relevance score for a paper"""
        try:
            score = 0.0

            # Base score from citation count
            score += min((paper.citation_count or 0) / 100, 0.3)

            # Recency bonus
            years_old = datetime.now(timezone.utc).year - (paper.publication_year or datetime.now(timezone.utc).year)
            recency_score = max(0, (5 - years_old) / 5) * 0.2
            score += recency_score

            # Domain relevance
            primary_domains = user_profile.get("primary_domains", [])
            title_lower = (paper.title or "").lower()

            for domain in primary_domains[:3]:
                if domain.lower() in title_lower:
                    score += 0.3
                    break

            # Activity level adjustment
            activity_level = user_profile.get("activity_level", "moderate")
            if activity_level == "high":
                score += 0.1
            elif activity_level == "low":
                score -= 0.1

            return min(score, 1.0)

        except Exception as e:
            logger.error(f"Error calculating personalized relevance: {e}")
            return 0.5

    # Additional helper methods
    def _calculate_interdisciplinary_score(self, paper: Article, domain1: str, domain2: str) -> float:
        """Calculate interdisciplinary relevance score"""
        try:
            title_lower = (paper.title or "").lower()
            abstract_lower = (paper.abstract or "").lower()

            # Check presence of both domains
            domain1_present = domain1.lower() in title_lower or domain1.lower() in abstract_lower
            domain2_present = domain2.lower() in title_lower or domain2.lower() in abstract_lower

            if domain1_present and domain2_present:
                base_score = 0.8
            elif domain1_present or domain2_present:
                base_score = 0.4
            else:
                base_score = 0.1

            # Citation bonus
            citation_bonus = min((paper.citation_count or 0) / 50, 0.2)

            return min(base_score + citation_bonus, 1.0)

        except Exception as e:
            logger.error(f"Error calculating interdisciplinary score: {e}")
            return 0.3

    def _calculate_citation_opportunity_score(self, paper: Article, user_profile: Dict) -> float:
        """Calculate citation opportunity score"""
        try:
            score = 0.0

            # Recency is key for citation opportunities
            years_old = datetime.now(timezone.utc).year - (paper.publication_year or datetime.now(timezone.utc).year)
            if years_old == 0:  # Current year
                score += 0.5
            elif years_old == 1:  # Last year
                score += 0.3

            # Low citation count means opportunity
            citation_count = paper.citation_count or 0
            if citation_count < 5:
                score += 0.3
            elif citation_count < 15:
                score += 0.2

            # Domain relevance
            primary_domains = user_profile.get("primary_domains", [])
            title_lower = (paper.title or "").lower()

            for domain in primary_domains:
                if domain.lower() in title_lower:
                    score += 0.2
                    break

            return min(score, 1.0)

        except Exception as e:
            logger.error(f"Error calculating citation opportunity score: {e}")
            return 0.3

    def _generate_cover_color(self, domain: str) -> str:
        """Generate Spotify-style cover colors for domains"""
        color_map = {
            "machine learning": "#1db954",
            "biology": "#1ed760",
            "medicine": "#ff6b6b",
            "physics": "#4ecdc4",
            "chemistry": "#45b7d1",
            "neuroscience": "#96ceb4",
            "engineering": "#feca57",
            "psychology": "#ff9ff3"
        }
        return color_map.get(domain.lower(), "#1db954")

    # Simplified implementations for missing methods
    async def _analyze_topic_preferences(self, saved_articles: List, db: Session) -> Dict:
        return {}

    def _calculate_activity_level(self, saved_articles: List) -> str:
        if len(saved_articles) > 50:
            return "high"
        elif len(saved_articles) > 20:
            return "moderate"
        else:
            return "low"

    def _analyze_discovery_preference(self, saved_articles: List, db: Session) -> str:
        return "balanced"

    def _calculate_collaboration_score(self, saved_articles: List, db: Session) -> float:
        return 0.5

    def _calculate_research_velocity(self, saved_articles: List) -> float:
        if not saved_articles:
            return 0.0

        # Calculate papers per week over last 3 months
        now_utc = datetime.now(timezone.utc)
        recent_articles = [a for a in saved_articles if
                          (now_utc - a.added_at).days <= 90]
        return len(recent_articles) / 12  # 12 weeks in 3 months

    def _calculate_recency_bias(self, saved_articles: List) -> float:
        return 0.7  # Default moderate recency bias

    async def _analyze_collection_themes(self, user_id: str, project_id: Optional[str], db: Session) -> List[str]:
        return []

    def _analyze_organization_style(self, user_id: str, project_id: Optional[str], db: Session) -> str:
        return "thematic"

    async def _enhance_papers_with_semantic_analysis(self, papers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Enhance papers with semantic analysis features

        Args:
            papers: List of paper dictionaries

        Returns:
            Enhanced papers with semantic analysis data
        """
        if not self.semantic_service:
            logger.warning("üîç Semantic analysis service not available, returning papers without enhancement")
            return papers

        enhanced_papers = []

        for paper in papers:
            try:
                # Ensure paper is a dictionary
                if not isinstance(paper, dict):
                    logger.warning(f"‚ö†Ô∏è Skipping non-dictionary paper object: {type(paper)}")
                    enhanced_papers.append(paper)
                    continue

                # Create cache key for this paper
                cache_key = f"semantic_{paper.get('pmid', '')}"

                # Check cache first
                if cache_key in self.semantic_cache:
                    semantic_features = self.semantic_cache[cache_key]
                    logger.debug(f"üìã Using cached semantic analysis for paper: {paper.get('title', 'Unknown')[:50]}...")
                else:
                    # Perform semantic analysis
                    title = paper.get('title', '')
                    abstract = paper.get('abstract', '')

                    if title and abstract:
                        logger.info(f"üß† Analyzing paper: {title[:50]}...")
                        semantic_features = await self.semantic_service.analyze_paper(title, abstract)

                        # Cache the result
                        self.semantic_cache[cache_key] = semantic_features
                        logger.debug(f"üíæ Cached semantic analysis for: {title[:50]}...")
                    else:
                        logger.warning(f"‚ö†Ô∏è Skipping semantic analysis for paper with missing title/abstract: {paper.get('pmid', 'Unknown')}")
                        semantic_features = None

                # Create enhanced paper with semantic features
                enhanced_paper = paper.copy()

                if semantic_features:
                    enhanced_paper.update({
                        'semantic_analysis': {
                            'methodology': semantic_features.methodology.value,
                            'complexity_score': semantic_features.complexity_score,
                            'novelty_classification': semantic_features.novelty_classification.value,
                            'research_domains': semantic_features.research_domains,
                            'technical_terms': semantic_features.technical_terms,
                            'confidence_scores': {
                                'methodology': semantic_features.confidence_scores.methodology,
                                'complexity': semantic_features.confidence_scores.complexity,
                                'novelty': semantic_features.confidence_scores.novelty,
                                'domains': semantic_features.confidence_scores.domains
                            }
                        }
                    })
                    logger.debug(f"‚úÖ Enhanced paper with semantic analysis: {title[:50]}...")
                else:
                    # Add empty semantic analysis to maintain consistency
                    enhanced_paper['semantic_analysis'] = None
                    logger.debug(f"‚ûñ Paper without semantic analysis: {paper.get('title', 'Unknown')[:50]}...")

                enhanced_papers.append(enhanced_paper)

            except Exception as e:
                logger.error(f"‚ùå Error enhancing paper with semantic analysis: {e}")
                # Add paper without semantic enhancement
                if isinstance(paper, dict):
                    enhanced_paper = paper.copy()
                    enhanced_paper['semantic_analysis'] = None
                    enhanced_papers.append(enhanced_paper)
                else:
                    enhanced_papers.append(paper)

        logger.info(f"üéØ Enhanced {len(enhanced_papers)} papers with semantic analysis")
        return enhanced_papers

    def _apply_semantic_filtering(self, papers: List[Dict[str, Any]], user_preferences: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Apply semantic-based filtering to papers based on user preferences

        Args:
            papers: List of papers with semantic analysis
            user_preferences: User preference data

        Returns:
            Filtered and ranked papers
        """
        if not papers:
            return papers

        # Apply semantic-enhanced scoring to each paper
        scored_papers = []
        for paper in papers:
            semantic_score = self._calculate_semantic_relevance_score(paper, user_preferences)
            paper_with_score = paper.copy()
            paper_with_score['semantic_relevance_score'] = semantic_score
            scored_papers.append(paper_with_score)

        # Sort by semantic relevance score
        scored_papers.sort(key=lambda p: p.get('semantic_relevance_score', 0.0), reverse=True)

        logger.info(f"üéØ Applied semantic filtering to {len(papers)} papers, top score: {scored_papers[0].get('semantic_relevance_score', 0.0):.3f}")
        return scored_papers

    def _calculate_semantic_relevance_score(self, paper: Dict[str, Any], user_preferences: Dict[str, Any]) -> float:
        """Calculate semantic-enhanced relevance score for a paper"""
        semantic_analysis = paper.get('semantic_analysis', {})
        if not semantic_analysis:
            return 0.5  # Default score for papers without semantic analysis

        score = 0.0

        # Methodology preference matching (25%)
        preferred_methodologies = dict(user_preferences.get('preferred_methodologies', []))
        paper_methodology = semantic_analysis.get('methodology')
        if paper_methodology in preferred_methodologies:
            methodology_score = preferred_methodologies[paper_methodology] / sum(preferred_methodologies.values())
            score += methodology_score * 0.25

        # Complexity preference matching (20%)
        user_complexity_pref = user_preferences.get('complexity_preference', 0.5)
        paper_complexity = semantic_analysis.get('complexity_score', 0.5)
        complexity_diff = abs(user_complexity_pref - paper_complexity)
        complexity_score = max(0, 1 - complexity_diff * 2)  # Closer = higher score
        score += complexity_score * 0.20

        # Novelty preference (15%)
        novelty_distribution = user_preferences.get('novelty_distribution', {})
        paper_novelty = semantic_analysis.get('novelty_classification', {}).get('value') if isinstance(semantic_analysis.get('novelty_classification'), dict) else semantic_analysis.get('novelty_classification')
        if paper_novelty in novelty_distribution:
            novelty_score = novelty_distribution[paper_novelty] / sum(novelty_distribution.values())
            score += novelty_score * 0.15

        # Domain relevance (25%)
        user_domains = dict(user_preferences.get('semantic_domains', []))
        paper_domains = semantic_analysis.get('research_domains', [])
        domain_score = 0
        for domain in paper_domains:
            if domain in user_domains:
                domain_score += user_domains[domain] / sum(user_domains.values())
        score += min(domain_score, 1.0) * 0.25

        # Base relevance (15%)
        base_score = paper.get('relevance_score', 0.5)
        score += base_score * 0.15

        return min(score, 1.0)

    def _apply_global_deduplication(self, recommendations: Dict[str, Any]) -> Dict[str, Any]:
        """Apply global deduplication across all recommendation types"""
        try:
            # Track seen PMIDs globally across all recommendation types
            seen_pmids = set()
            deduplicated = {}

            # Process in priority order: Papers-for-you > Trending > Cross-pollination > Citation opportunities
            # This ensures personalized content gets priority, then trending, then diverse content
            priority_order = ['papers_for_you', 'trending_in_field', 'cross_pollination', 'citation_opportunities']

            for rec_type in priority_order:
                if rec_type in recommendations and 'papers' in recommendations[rec_type]:
                    original_papers = recommendations[rec_type]['papers']

                    # First, deduplicate within this category by PMID
                    category_seen_pmids = set()
                    category_deduplicated = []

                    for paper in original_papers:
                        pmid = paper.get('pmid')
                        if pmid and pmid not in category_seen_pmids:
                            category_deduplicated.append(paper)
                            category_seen_pmids.add(pmid)

                    logger.info(f"üîß {rec_type}: Reduced from {len(original_papers)} to {len(category_deduplicated)} papers (within-category dedup)")

                    # Then, apply global deduplication
                    global_deduplicated = []
                    for paper in category_deduplicated:
                        pmid = paper.get('pmid')
                        if pmid and pmid not in seen_pmids:
                            global_deduplicated.append(paper)
                            seen_pmids.add(pmid)

                    # Ensure each category has at least some papers (but not duplicates)
                    if len(global_deduplicated) == 0 and len(category_deduplicated) > 0:
                        # If all papers were global duplicates, take the first unique one anyway
                        global_deduplicated = category_deduplicated[:1]
                        logger.info(f"üîÑ {rec_type}: All papers were global duplicates, keeping first 1 for variety")

                    # Preserve the original structure but with deduplicated papers
                    deduplicated[rec_type] = {
                        **recommendations[rec_type],
                        'papers': global_deduplicated
                    }

                    logger.info(f"‚úÖ {rec_type}: Final count = {len(global_deduplicated)} papers")
                else:
                    deduplicated[rec_type] = recommendations.get(rec_type, {'papers': []})

            logger.info(f"üîÑ Global deduplication completed. Removed {len([p for rec in recommendations.values() if 'papers' in rec for p in rec['papers']]) - len(seen_pmids)} duplicates")

            return deduplicated

        except Exception as e:
            logger.error(f"Error in global deduplication: {e}")
            return recommendations  # Return original if deduplication fails

    def _get_deduplication_stats(self, original: Dict[str, Any], deduplicated: Dict[str, Any]) -> Dict[str, Any]:
        """Get statistics about the deduplication process"""
        try:
            original_counts = {}
            deduplicated_counts = {}

            for rec_type in ['cross_pollination', 'trending', 'papers_for_you', 'citation_opportunities']:
                original_counts[rec_type] = len(original.get(rec_type, {}).get('papers', []))
                deduplicated_counts[rec_type] = len(deduplicated.get(rec_type, {}).get('papers', []))

            total_original = sum(original_counts.values())
            total_deduplicated = sum(deduplicated_counts.values())

            return {
                'original_counts': original_counts,
                'deduplicated_counts': deduplicated_counts,
                'total_duplicates_removed': total_original - total_deduplicated,
                'deduplication_rate': (total_original - total_deduplicated) / total_original if total_original > 0 else 0
            }

        except Exception as e:
            logger.error(f"Error calculating deduplication stats: {e}")
            return {}

    async def _generate_subject_area_recommendations(self, subject_area: str, db: Session) -> Dict[str, Any]:
        """Generate recommendations based on user's registered subject area"""
        try:
            logger.info(f"üéì Generating recommendations based on subject area: {subject_area}")

            # Map subject areas to search keywords
            subject_keywords = {
                "medicine": ["medical", "clinical", "patient", "treatment", "therapy", "disease"],
                "biology": ["biological", "cell", "gene", "protein", "organism", "molecular"],
                "computer science": ["algorithm", "machine learning", "AI", "software", "computing"],
                "chemistry": ["chemical", "molecule", "compound", "reaction", "synthesis"],
                "physics": ["physics", "quantum", "particle", "energy", "matter"],
                "engineering": ["engineering", "design", "system", "optimization", "technology"],
                "psychology": ["psychology", "behavior", "cognitive", "mental", "brain"],
                "neuroscience": ["neuroscience", "brain", "neural", "neurological", "cognitive"]
            }

            # Get keywords for the subject area
            keywords = subject_keywords.get(subject_area.lower(), ["research", "study", "analysis"])

            # Generate papers for the subject area
            papers_list = []

            # Find papers for each keyword
            for keyword in keywords[:3]:  # Use top 3 keywords
                papers = db.query(Article).filter(
                    or_(
                        Article.title.ilike(f'%{keyword}%'),
                        Article.abstract.ilike(f'%{keyword}%')
                    ),
                    Article.citation_count > 10,
                    Article.title.isnot(None),
                    Article.title != ""
                ).order_by(desc(Article.citation_count)).limit(5).all()

                for paper in papers:
                    if len(papers_list) < 8:
                        papers_list.append({
                            "pmid": paper.pmid,
                            "title": paper.title,
                            "authors": paper.authors[:3] if paper.authors else [],
                            "journal": paper.journal,
                            "year": paper.publication_year,
                            "citation_count": paper.citation_count or 0,
                            "relevance_score": 0.85,
                            "reason": f"Relevant to your {subject_area} background",
                            "category": "papers_for_you"
                        })

            logger.info(f"üéì Generated {len(papers_list)} subject-area recommendations")

            # Return in the expected format for weekly recommendations
            return {
                "papers_for_you": {
                    "title": "Papers for You",
                    "description": f"Research papers relevant to your {subject_area} background",
                    "papers": papers_list,
                    "updated": datetime.now(timezone.utc).isoformat(),
                    "refresh_reason": f"Based on your {subject_area} subject area"
                },
                "trending_in_field": {
                    "title": "Trending in Field",
                    "papers": [],
                    "updated": datetime.now(timezone.utc).isoformat()
                },
                "cross_pollination": {
                    "title": "Cross-Domain Insights",
                    "papers": [],
                    "updated": datetime.now(timezone.utc).isoformat()
                },
                "citation_opportunities": {
                    "title": "Citation Opportunities",
                    "papers": [],
                    "updated": datetime.now(timezone.utc).isoformat()
                }
            }

        except Exception as e:
            logger.error(f"Error generating subject area recommendations: {e}")
            return {
                "papers_for_you": {
                    "title": "Papers for You",
                    "papers": [],
                    "error": str(e)
                },
                "trending_in_field": {
                    "title": "Trending in Field",
                    "papers": []
                },
                "cross_pollination": {
                    "title": "Cross-Domain Insights",
                    "papers": []
                },
                "citation_opportunities": {
                    "title": "Citation Opportunities",
                    "papers": []
                }
            }

# Global service instance
_spotify_recommendations_service = None

def get_spotify_recommendations_service() -> SpotifyInspiredRecommendationsService:
    """Get the global Spotify-inspired recommendations service instance"""
    global _spotify_recommendations_service
    if _spotify_recommendations_service is None:
        _spotify_recommendations_service = SpotifyInspiredRecommendationsService()
    return _spotify_recommendations_service

# Backward compatibility
def get_ai_recommendations_service() -> SpotifyInspiredRecommendationsService:
    """Backward compatibility alias"""
    return get_spotify_recommendations_service()
