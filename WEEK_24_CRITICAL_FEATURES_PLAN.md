# Week 24: Critical Features Implementation Plan

## ğŸ¯ **LESSONS LEARNED FROM EXPERIMENT PLANNER REGRESSION**

### **What Went Wrong:**
1. âŒ Multi-agent system generated LESS detail than legacy system
2. âŒ 5 critical fields were completely empty
3. âŒ No validation to ensure output quality matched legacy
4. âŒ No A/B testing before full deployment

### **What We'll Do Differently:**
1. âœ… **Validate output quality** - Compare new vs old, ensure >= same detail
2. âœ… **Feature flags** - Deploy behind flags, test thoroughly before enabling
3. âœ… **Comprehensive testing** - Test all edge cases, compare outputs
4. âœ… **Incremental rollout** - One feature at a time, validate each
5. âœ… **Regression tests** - Automated tests to catch quality degradation

---

## ğŸ“‹ **IMPLEMENTATION ORDER**

### **Phase 1: Quick Wins (Day 1)** âš¡
1. Show evidence count badges on hypotheses (30 min)
2. Add "Last updated" timestamps everywhere (1 hour)
3. Improve error messages with actionable guidance (2 hours)
4. Add loading states for all async operations (2 hours)

**Total: 5.5 hours | Risk: LOW | Impact: HIGH**

---

### **Phase 2: Tables & Figures Display (Days 2-3)** ğŸ–¼ï¸
1. Add PDF fields to inbox API response
2. Verify migration 011 ran on Railway
3. Fix frontend rendering for base64 images
4. Add "Re-extract PDF" button in UI

**Success Criteria:**
- âœ… All papers with PDF data show tables/figures in UI
- âœ… Base64 images render correctly
- âœ… Re-extract button works and shows progress
- âœ… No performance degradation (< 2s load time)

**Testing:**
- Test with paper that has tables (PMID: 35650602)
- Test with paper that has figures
- Test with paper that has both
- Test with paper that has neither
- Test re-extraction flow

---

### **Phase 3: Auto Evidence Linking (Days 4-6)** ğŸ”—
1. Create evidence linking service
2. Auto-create hypothesis_evidence records after triage
3. Update evidence counts automatically
4. Add feature flag: `AUTO_EVIDENCE_LINKING`

**Success Criteria:**
- âœ… Evidence links created automatically after triage
- âœ… Evidence counts updated correctly
- âœ… No duplicate evidence links
- âœ… User can still manually add/edit evidence
- âœ… Evidence quality >= manual linking

**Testing:**
- Test with paper that supports hypothesis
- Test with paper that contradicts hypothesis
- Test with paper that provides context
- Test with paper that's not relevant
- Test duplicate prevention
- Compare auto-linked vs manually-linked evidence quality

---

### **Phase 4: Auto Hypothesis Status Updates (Days 7-8)** ğŸ“Š
1. Define evidence thresholds
2. Create status update service
3. Auto-update status when evidence added
4. Add feature flag: `AUTO_HYPOTHESIS_STATUS`

**Success Criteria:**
- âœ… Status updates automatically based on evidence
- âœ… Confidence level calculated correctly
- âœ… User can override with explanation
- âœ… Status history tracked
- âœ… No false positives (wrong status)

**Testing:**
- Test with 0 evidence (should stay "proposed")
- Test with 1-2 supporting (should be "testing")
- Test with 3+ supporting (should be "supported")
- Test with 3+ contradicting (should be "rejected")
- Test with mixed evidence (should be "inconclusive")
- Test user override

---

### **Phase 5: Smart Recommendations (Days 9-14)** ğŸ¤–
1. Create recommendations service
2. Implement paper recommendations
3. Implement experiment recommendations
4. Add feature flag: `SMART_RECOMMENDATIONS`

**Success Criteria:**
- âœ… Recommendations are relevant (>80% user acceptance)
- âœ… Recommendations update in real-time
- âœ… No duplicate recommendations
- âœ… Performance < 1s for recommendations
- âœ… User can dismiss/hide recommendations

**Testing:**
- Test with new project (no data)
- Test with project with papers only
- Test with project with hypotheses only
- Test with project with experiments
- Test recommendation quality (manual review)

---

## ğŸ§ª **TESTING STRATEGY**

### **1. Unit Tests**
- Test each service function independently
- Mock dependencies
- Test edge cases

### **2. Integration Tests**
- Test full workflow end-to-end
- Test with real data
- Test error handling

### **3. Regression Tests**
- Compare output quality before/after
- Ensure no fields are empty
- Ensure detail level >= legacy

### **4. Performance Tests**
- Measure response times
- Ensure < 2s for API calls
- Ensure < 1s for UI updates

### **5. User Acceptance Tests**
- Test with real user workflows
- Get feedback on quality
- Iterate based on feedback

---

## ğŸš¨ **ROLLBACK PLAN**

If any feature causes issues:
1. **Disable feature flag** immediately
2. **Investigate root cause** in logs
3. **Fix issue** in development
4. **Re-test thoroughly** before re-enabling
5. **Document issue** and prevention

---

## ğŸ“Š **SUCCESS METRICS**

### **Phase 1: Quick Wins**
- âœ… Evidence badges visible on all hypotheses
- âœ… Timestamps visible on all entities
- âœ… Error messages are actionable
- âœ… Loading states show for all async ops

### **Phase 2: Tables & Figures**
- âœ… 100% of papers with PDF data show tables/figures
- âœ… 0 rendering errors
- âœ… < 2s load time

### **Phase 3: Auto Evidence Linking**
- âœ… 100% of relevant evidence auto-linked
- âœ… 0 duplicate links
- âœ… Evidence quality score >= 8/10

### **Phase 4: Auto Hypothesis Status**
- âœ… 100% of hypotheses have correct status
- âœ… 0 false positives
- âœ… Status updates within 1s

### **Phase 5: Smart Recommendations**
- âœ… >80% user acceptance rate
- âœ… < 1s recommendation generation
- âœ… >90% relevance score

---

## ğŸ¯ **IMPLEMENTATION CHECKLIST**

### **Before Starting Each Phase:**
- [ ] Review success criteria
- [ ] Create feature flag
- [ ] Write tests first (TDD)
- [ ] Document expected behavior

### **During Implementation:**
- [ ] Follow existing patterns
- [ ] Add comprehensive logging
- [ ] Handle all error cases
- [ ] Test incrementally

### **After Implementation:**
- [ ] Run all tests
- [ ] Compare output quality
- [ ] Test with real data
- [ ] Get user feedback
- [ ] Document changes

---

## ğŸ“ **NEXT STEPS**

1. **Start with Phase 1** (Quick Wins) - Low risk, high impact
2. **Deploy and validate** each phase before moving to next
3. **Monitor metrics** continuously
4. **Iterate based on feedback**

Let's begin! ğŸš€

